{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9418 - Assignment 1 - Bayesian Networks as Classifiers\n",
    "\n",
    "## UNSW Sydney, October 2020\n",
    "\n",
    "- Minrui Lu z5277884\n",
    "- Yangqi Zhang z5235062"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "**Submission deadline:** Sunday, 18th October 2020, at 18:00:00.\n",
    "\n",
    "**Late Submission Policy:** The penalty is set at 20% per late day. This is ceiling penalty, so if a group is marked 60/100 and they submitted two days late, they still get 60/100.\n",
    "\n",
    "**Form of Submission:** This is a group assignment. Each group can have up to **two** students. **Only one member of the group should submit the assignment**.\n",
    "\n",
    "You can reuse any piece of source code developed in the tutorials.\n",
    "\n",
    "Submit your files using give. On a CSE Linux machine, type the following on the command-line:\n",
    "\n",
    "``$ give cs9418 ass1 solution.zip``\n",
    "\n",
    "Alternative, you can submit your solution via the [WebCMS](https://webcms3.cse.unsw.edu.au/COMP9418/20T3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical prerequisites\n",
    "\n",
    "These are the libraries your are allowed to use. No other libraries will be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make division default to floating-point, saving confusion\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Allowed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import heapq as pq\n",
    "import matplotlib as mp\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "from collections import OrderedDict as odict\n",
    "from graphviz import Digraph\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Supplemental libraries\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial task - Initialise graph\n",
    "\n",
    "Create a graph ``G`` that represents the following network by filling in the edge lists.\n",
    "![Bayes Net](BayesNet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = {\n",
    "    \"BreastDensity\" : [\"Mass\"],\n",
    "    \"Location\" : [\"BC\"],\n",
    "    \"Age\" : [\"BC\"],\n",
    "    \"BC\" : [\"Mass\", \"AD\", \"Metastasis\", \"MC\", \"SkinRetract\",\"NippleDischarge\"],\n",
    "    \"Mass\" : [\"Size\",  \"Shape\", \"Margin\" ],\n",
    "    \"AD\" : [\"FibrTissueDev\"],\n",
    "    \"Metastasis\" : [ \"LymphNodes\"],\n",
    "    \"MC\" : [],\n",
    "    \"Size\" : [],\n",
    "    \"Shape\" : [],\n",
    "    \"FibrTissueDev\" : [ \"SkinRetract\" , \"NippleDischarge\",\"Spiculation\" ],\n",
    "    \"LymphNodes\" : [],\n",
    "    \"SkinRetract\" : [],\n",
    "    \"NippleDischarge\" : [],\n",
    "    \"Spiculation\" : [\"Margin\" ],\n",
    "    \"Margin\" : [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 1 - Efficient d-separation test\n",
    "\n",
    "Implement the efficient version of the d-separation algorithm in a function ``d_separation(G, X, Z, Y)`` that return a boolean: true if **X** is d-separated from **Y** given **Z** in the graph $G$ and false otherwise.\n",
    "\n",
    "* **X**,**Y** and **Z** are python sets, each containing a set of variable names. \n",
    "* Variable names may be strings or integers, and can be assumed to be nodes of the graph $G$. \n",
    "* $G$ is a graph as defined in tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "def isleaf_node(G,node):\n",
    "    return not G[node] \n",
    "\n",
    "# delete the leaf node and its edges from the G, return a new Graph\n",
    "def remove_leaf(G1,leaf_node):\n",
    "    G_new = copy.deepcopy(G1)\n",
    "    del G_new[leaf_node]\n",
    "    for key, value in  G_new.items(): \n",
    "        if leaf_node in value:\n",
    "            G_new[key].remove(leaf_node)\n",
    "    return G_new\n",
    "    \n",
    "def repeat_del(G1,node_list): \n",
    "    count = 1 \n",
    "    list_update = node_list.copy()\n",
    "    while count > 0:\n",
    "        count = 0\n",
    "        for node in node_list: \n",
    "            if isleaf_node(G1,node):\n",
    "                #remove the nodes and update the graph \n",
    "                G1= copy.deepcopy(remove_leaf(G1,node))\n",
    "                list_update.remove(node)\n",
    "                count = count + 1 \n",
    "        node_list = list_update.copy()\n",
    "    return (G1)\n",
    "\n",
    "    \n",
    "def dfs_r(G, v, colour):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `G`, an adjacency list representation of a graph\n",
    "    `v`, next vertex to be visited\n",
    "    `colour`, dictionary with the colour of each node\n",
    "    \"\"\"\n",
    "    # Visited vertices are coloured 'grey'\n",
    "    colour[v] = 'grey'\n",
    "    # Let's visit all outgoing edges from v\n",
    "    for w in G[v]:\n",
    "        # To avoid loops, we vist check if the next vertex hasn't been visited yet\n",
    "        if colour[w] == 'white':\n",
    "            dfs_r(G, w, colour)\n",
    "    # When we finish the for loop, we know we have visited all nodes from v. It is time to turn it 'black'\n",
    "    colour[v] = 'black' \n",
    "    return None\n",
    "  \n",
    "\n",
    "\n",
    "def d_separation(G1, X, Z, Y): \n",
    "    \"\"\" \n",
    "    Arguments: \n",
    "    `G`, an adjacency list representation of a graph \n",
    "    `X`, a set of variables name \n",
    "    `Y`, a set of variables name \n",
    "    `Z`, a set of a set of variables name \n",
    "    \n",
    "    Returns \n",
    "    a boolean: true if X is d-separated from Y given Z in the graph  ùê∫  and false otherwise.\n",
    "    \n",
    "    \"\"\"\n",
    "    if bool(X.intersection(Y).intersection(Z)):\n",
    "        raise Exception(\"X, Y, Z are not disjoint\") \n",
    "    \n",
    "    combine_set = X.union(Y).union(Z)\n",
    "    node_set = set(G1.keys())\n",
    "    remain_nodes = set(node_set  - combine_set)\n",
    "    \n",
    "    G_final = copy.deepcopy(repeat_del(G1,remain_nodes)) \n",
    "    \n",
    "    for var in Z: \n",
    "        G_final[var] = [] \n",
    "    \n",
    "    for key,values in G_final.items():\n",
    "        if bool(values):\n",
    "            for n in values: \n",
    "                G_final[n].append(key) \n",
    "\n",
    "    colour = {node: 'white' for node in G_final.keys()}\n",
    "    #check connectivity \n",
    "    separate = True \n",
    "    for nodex in X:\n",
    "        dfs_r(G_final,nodex,colour) \n",
    "        Y_color = [colour[node] for node in Y]\n",
    "        if 'black' in Y_color:\n",
    "            separate = False\n",
    "    return(separate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "def test(statement):\n",
    "    if statement:\n",
    "        print(\"Passed test case\")\n",
    "    else:\n",
    "        print(\"Failed test case\")\n",
    "        \n",
    "test(d_separation(G, set(['Age']), set(['BC']), set(['AD'])))\n",
    "test(not d_separation(G, set(['Spiculation','LymphNodes']), set(['MC', 'Size']), set(['Age'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Marks] Task 2 - Estimate Bayesian Network parameters from data\n",
    "\n",
    "Implement a function ``learn_outcome_space(data)`` that learns the outcome space (the valid values for each variable) from the pandas dataframe ``data`` and returns a dictionary ``outcomeSpace`` with these values.\n",
    "\n",
    "Implement a function ``learn_bayes_net(G, data, outcomeSpace)`` that learns the parameters of the Bayesian Network $G$. This function should return a dictionary ``prob_tables`` with the all conditional probability tables (one for each node).\n",
    "\n",
    "- ``G`` is a directed acyclic graph. For this part of the assignment, $G$ should be declared according to the breast cancer Bayesian network presented in the diagram in the assignment specification.\n",
    "- ``data`` is a dataframe created from a csv file containing the relevant data. \n",
    "- ``outcomeSpace`` is defined in tutorials.\n",
    "- ``prob_tables`` is a dict from each variable name (node) to a \"factor\". Factors are defined in tutorial 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_outcome_space(data) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_outcome_space(data):\n",
    "    outcomeSpace = {}\n",
    "    for attr in data.columns:\n",
    "        outcomeSpace[attr] = tuple(data[attr].unique())\n",
    "    return(outcomeSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "with open('bc.csv') as file:\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "outcomeSpace = learn_outcome_space(data)\n",
    "\n",
    "outcomes = outcomeSpace['BreastDensity']\n",
    "answer = ('high', 'medium', 'low')\n",
    "test(len(outcomes) == len(answer) and set(outcomes) == set(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_bayes_net(G, data, outcomeSpace) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxilliary functions\n",
    "def printFactor(f):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, a factor to print on screen\n",
    "    \"\"\"\n",
    "    # Create a empty list that we will fill in with the probability table entries\n",
    "    table = list()\n",
    "    \n",
    "    # Iterate over all keys and probability values in the table\n",
    "    for key, item in f['table'].items():\n",
    "        k = list(key)\n",
    "        k.append(item)\n",
    "        table.append(k)\n",
    "    dom = list(f['dom'])\n",
    "    dom.append('Pr')\n",
    "    print(tabulate(table,headers=dom,tablefmt='fancy_grid'))\n",
    "    \n",
    "def prob(factor, *entry):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `factor`, a dictionary of domain and probability values,\n",
    "    `entry`, a list of values, one for each variable in the same order as specified in the factor domain.\n",
    "    \n",
    "    Returns p(entry)\n",
    "    \"\"\"\n",
    "    return factor['table'][entry]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allEqualThisIndex(dict_of_arrays,**fixed_vars):\n",
    "    first_array = dict_of_arrays[list(dict_of_arrays.keys())[0]]\n",
    "    index = np.ones_like(first_array,dtype=np.bool_)\n",
    "    for var_name,var_val in fixed_vars.items():\n",
    "        index = index & (np.asarray(dict_of_arrays[var_name])==var_val)\n",
    "    return (index)\n",
    "\n",
    "def estProbTable(data,var_name,parent_names,outcomeSpace,alpha=1):\n",
    "    var_outcomes = outcomeSpace[var_name]\n",
    "    parent_outcomes = [outcomeSpace[var] for var in parent_names]\n",
    "    all_parent_combinations = product(*parent_outcomes)\n",
    "    prob_table = odict()\n",
    "    \n",
    "    for i,parent_combination in enumerate(all_parent_combinations):\n",
    "        parent_vars = dict(zip(parent_names,parent_combination))\n",
    "        parent_index = allEqualThisIndex(data,**parent_vars)\n",
    "        for var_outcome in var_outcomes:\n",
    "            var_index = (np.asarray(data[var_name])==var_outcome)\n",
    "            new_dom = tuple(list(parent_combination)+[var_outcome])\n",
    "\n",
    "            # Additive smoothing is applied here\n",
    "            \n",
    "            N = parent_index.sum()\n",
    "            c = (var_index & parent_index).sum()\n",
    "            X_cardinality = len(var_outcomes)\n",
    "            prob_table[new_dom] = (c+alpha)/(N+alpha*X_cardinality)\n",
    "            \n",
    "    return({'dom':tuple(list(parent_names)+[var_name]),'table':prob_table})\n",
    "\n",
    "def transposeGraph(G):\n",
    "    GT = dict((v,[]) for v in G)\n",
    "    for v in G:\n",
    "        for w in G[v]:\n",
    "            GT[w].append(v)\n",
    "    return (GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_bayes_net(G,data,outcomeSpace):\n",
    "    bayes_net = odict()\n",
    "    GT = transposeGraph(G)\n",
    "    for child, parents in GT.items():\n",
    "        bayes_net[child] = estProbTable(data,child,parents,outcomeSpace)\n",
    "    return(bayes_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "prob_tables = learn_bayes_net(G, data, outcomeSpace)\n",
    "test(abs(prob_tables['Age']['table'][('35-49',)] - 0.2476) < 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 3 - Bayesian Network Classification\n",
    "\n",
    "Design a new function ``assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)`` that uses the test cases in ``data`` to assess the performance of the Bayesian network defined by ``G`` and ``prob_tables``. Implement the efficient classification procedure discussed in the lectures. Such a function should return the classifier accuracy. \n",
    " * ``class_var`` is the name of the variable you are predicting, using all other variables.\n",
    " * ``outcomeSpace`` was created in task 2\n",
    " \n",
    "Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy.\n",
    "\n",
    "Return just the accuracy:\n",
    "\n",
    "``acc = assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var) in one or more cells here\n",
    "def join(f1, f2, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f1`, first factor to be joined.\n",
    "    `f2`, second factor to be joined.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor with a join of f1 and f2\n",
    "    \"\"\"\n",
    "    common_vars = list(f1['dom']) + list(set(f2['dom']) - set(f1['dom']))\n",
    "    table = list()\n",
    "    for entries in product(*[outcomeSpace[node] for node in common_vars]):\n",
    "        entryDict = dict(zip(common_vars, entries))\n",
    "        f1_entry = (entryDict[var] for var in f1['dom'])\n",
    "        f2_entry = (entryDict[var] for var in f2['dom'])\n",
    "        p1 = prob(f1, *f1_entry)          \n",
    "        p2 = prob(f2, *f2_entry)           \n",
    "        table.append((entries, p1 * p2))\n",
    "    return {'dom': tuple(common_vars), 'table': odict(table)}\n",
    "\n",
    "def p_joint(outcomeSpace, cond_tables):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `outcomeSpace`, dictionary with domain of each variable\n",
    "    `cond_tables`, conditional probability distributions estimated from data\n",
    "    \n",
    "    Returns a new factor with full joint distribution\n",
    "    \"\"\"       \n",
    "    var_list = list(outcomeSpace.keys())\n",
    "    p = join(cond_tables[var_list[0]], cond_tables[var_list[1]], outcomeSpace)\n",
    "\n",
    "    for var in var_list[2:]:\n",
    "        p = join(p,cond_tables[var_list[var]],outcomeSpace)\n",
    "    return p\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a markov blanket of a given variable in the graph\n",
    "def markov_blanket(G,var):\n",
    "    blanket_list = []\n",
    "    blanket_list = blanket_list + G[var] #include the children\n",
    "    children_list = blanket_list \n",
    "    GT = transposeGraph(G)\n",
    "    blanket_list = blanket_list + GT[var] #include the parents     \n",
    "    for node in children_list:\n",
    "        blanket_list = blanket_list + GT[node] #include spouse \n",
    "    \n",
    "    blanket_list = list(set(blanket_list))\n",
    "    blanket_list = [i for i in blanket_list if i != var]\n",
    "    return blanket_list\n",
    "\n",
    "# Join each variable's markov blanket in the outcomeSpace together\n",
    "def p_joint_blanket(my_blanket, outcomeSpace, cond_tables):\n",
    "    var_list = my_blanket\n",
    "    p = join(cond_tables[var_list[0]], cond_tables[var_list[1]], outcomeSpace)\n",
    "    for var in var_list[2:]:\n",
    "        p = join(p,cond_tables[var], outcomeSpace)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidence(var, e, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `var`, a valid variable identifier.\n",
    "    `e`, the observed value for var.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns dictionary with a copy of outcomeSpace with var = e\n",
    "    \"\"\"    \n",
    "    newOutcomeSpace = outcomeSpace.copy()      \n",
    "    newOutcomeSpace[var] = (e,)               \n",
    "    return newOutcomeSpace\n",
    "\n",
    "def marginalize(f, var, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be marginalized.\n",
    "    `var`, variable to be summed out.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor f' with dom(f') = dom(f) - {var}\n",
    "    \"\"\"    \n",
    "    new_dom = list(f['dom'])\n",
    "    \n",
    "    new_dom.remove(var)           \n",
    "    table = list()                 \n",
    "    for entries in product(*[outcomeSpace[node] for node in new_dom]):\n",
    "        s = 0;                     \n",
    "        for val in outcomeSpace[var]:\n",
    "            entriesList = list(entries)\n",
    "            entriesList.insert(f['dom'].index(var), val)\n",
    "            p = prob(f, *tuple(entriesList))     \n",
    "            s = s + p                           \n",
    "        \n",
    "        table.append((entries, s))\n",
    "    return {'dom': tuple(new_dom), 'table': odict(table)}\n",
    "\n",
    "\n",
    "def normalize(f):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be normalized.\n",
    "    \n",
    "    Returns a new factor f' as a copy of f with entries that sum up to 1\n",
    "    \"\"\" \n",
    "    table = list()\n",
    "    sum = 0\n",
    "    for k, p in f['table'].items():\n",
    "        sum = sum + p\n",
    "    for k, p in f['table'].items():\n",
    "        table.append((k, p/sum))\n",
    "    return {'dom': f['dom'], 'table': odict(table)}\n",
    "\n",
    "\n",
    "def query(p, outcomeSpace, q_vars, **q_evi):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `p`, probability table to query.\n",
    "    `outcomeSpace`, dictionary will variable domains\n",
    "    `q_vars`, list of variables in query head\n",
    "    `q_evi`, dictionary of evidence in the form of variables names and values\n",
    "    \n",
    "    Returns a new factor NORMALIZED factor will all hidden variables eliminated as evidence set as in q_evi\n",
    "    \"\"\"     \n",
    "    pm = p.copy()\n",
    "    outSpace = outcomeSpace.copy()\n",
    "\n",
    "    for var_evi, e in q_evi.items():\n",
    "        outSpace = evidence(var_evi, e, outSpace)\n",
    "    for var in outSpace:\n",
    "        if not var in q_vars:\n",
    "            pm = marginalize(pm, var, outSpace)\n",
    "    return normalize(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the accuracy of a Bayes network given a data\n",
    "def assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var): \n",
    "    var_blanket = markov_blanket(G,class_var)\n",
    "    var_blanket.append(class_var)\n",
    "    var_remove = ['Metastasis', 'LymphNodes']\n",
    "    # now we get the variables that needs for inference class_var\n",
    "    var_list = [i for i in var_blanket if i not in var_remove]  \n",
    "    p_table =  p_joint_blanket(var_list, outcomeSpace, prob_tables) \n",
    "    q_var = class_var\n",
    "    evidence_list = [var for var in var_list if var!=class_var]\n",
    "    data_update = data[evidence_list]\n",
    "    data_dict = data_update.to_dict(orient='records')\n",
    "    new_outcomeSpace = { var: outcomeSpace[var] for var in var_list}\n",
    "    match_count = 0\n",
    "    for i in range(len(data_dict)):\n",
    "        q_table = query(p_table, new_outcomeSpace, q_var, **data_dict[i])\n",
    "        pred = max(q_table['table'],key=q_table['table'].get)[0]\n",
    "        if (pred == data.iloc[i][q_var]):\n",
    "            match_count +=1\n",
    "    return (match_count/data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "class_var = \"BC\"\n",
    "acc = assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8423"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a function ``cv_bayes_net(G, data, class_var)`` that uses ``learn_outcome_space``, ``learn_bayes_net``and ``assess_bayes_net`` to learn and assess a Bayesian network in a dataset using 10-fold cross-validation. Compute and report the average accuracy over the ten cross-validation runs as well as the standard deviation, e.g.\n",
    "\n",
    "``acc, stddev = cv_bayes_net(G, data, class_var)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_bayes_net(G, data, class_var) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross validation is 10 fold here\n",
    "def cv_bayes_net(G,data,class_var):\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    fold_len = int(data.shape[0]/10)\n",
    "    acc_list = []\n",
    "    for i in range(10):\n",
    "        training_index = list(range(0,i*fold_len)) + list(range((i+1)*fold_len,data.shape[0]))\n",
    "        test_index = list(range(i*fold_len,(i+1)*fold_len))\n",
    "        training_data = data.iloc[training_index]\n",
    "        test_data = data.iloc[test_index]\n",
    "        prob_tables = learn_bayes_net(G,training_data,outcomeSpace)\n",
    "        acc_list.append(assess_bayes_net(G,prob_tables,test_data,outcomeSpace,class_var))\n",
    "    return (np.mean(acc_list),np.std(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.8412, and the standard deviation is 0.01\n",
      "The time consumption is 6.76 seconds\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "begin = time.time()\n",
    "acc, stddev = cv_bayes_net(G, data, 'BC')\n",
    "end = time.time()\n",
    "print(\"The accuracy is {0:.4f}, and the standard deviation is {1:.2f}\".format(acc,stddev))\n",
    "print(\"The time consumption is {0:.2f} seconds\".format(end-begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Marks] Task 4 - Na√Øve Bayes Classification\n",
    "\n",
    "Design a new function ``assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var)`` to classify and assess the test cases in a dataset ``data`` according to the Na√Øve Bayes classifier. To classify each example, use the log probability trick discussed in the lectures. This function should return the accuracy of the classifier in ``data``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var) in one or more cells here\n",
    "\n",
    "# Return a Naive Bayes graph, in which there is only one parent, and other nodes are all its child\n",
    "def learn_naive_bayes_structure(outcomeSpace, class_var):\n",
    "    \"\"\"Return the naive-bayes graph structure (a dict) according to above info\"\"\"\n",
    "    G_nb = {}\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    node_list.remove(class_var)\n",
    "    G_nb[class_var] = node_list\n",
    "    for nodes in node_list:\n",
    "        G_nb[nodes] = []\n",
    "    return G_nb\n",
    "\n",
    "def additive_smoothing(prop_tables,data,alpha=1):\n",
    "    N = data.shape[0]\n",
    "    tables = copy.deepcopy(prop_tables)\n",
    "    for node , table_dict in tables.items():\n",
    "        X_cardinality = len(table_dict['table'].values())\n",
    "        if (0 in table_dict['table'].values()): # addtive smoothing required\n",
    "            for nameSpace, prob in table_dict['table'].items():\n",
    "                count_X_eq_x = int(round(prob * N,0))\n",
    "                table_dict['table'][nameSpace] = (count_X_eq_x+alpha)/(N+alpha*X_cardinality)\n",
    "    return (tables)\n",
    "\n",
    "#Return the log likelihood for each evidence variable\n",
    "def single_var_query( e, node_table):\n",
    "    # using log to avoid probability diminishing\n",
    "    prob_with_evi = {key[0]: np.log(value) for key,value in node_table['table'].items() if key[1] == e}    \n",
    "    return prob_with_evi\n",
    "\n",
    "# Given a line of data, return the class prediction\n",
    "def predict(x, y_space, table, prior):\n",
    "    # initialize the prediction dictionary by the prior probabilities\n",
    "    pre_dict = {i:  np.log(prior[i]) for i in y_space if prior[i]!=0}\n",
    "    for i in range(len(x)):\n",
    "        var_prob = single_var_query(x[i], table[x.index[i]])\n",
    "        for key in pre_dict.keys():\n",
    "            pre_dict[key] = pre_dict[key] +  var_prob[key]    \n",
    "    yhat = max(pre_dict, key=pre_dict.get)   \n",
    "    return yhat \n",
    "\n",
    "# Return the accuracy of a Naive Bayes network in the test data\n",
    "def assess_naive_bayes(G_naive, naive_tables, data, outcomeSpace, class_var):\n",
    "    naive_tables = additive_smoothing(naive_tables,data)   \n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    var_remove = ['Metastasis', 'LymphNodes']\n",
    "    var_list = [i for i in node_list if i not in var_remove] #now we get all the variables    \n",
    "    evidence_list = [var for var in var_list if var!=class_var]\n",
    "    data_update = data[evidence_list]\n",
    "    prior_prob = data[class_var].value_counts(normalize = True)   \n",
    "    y_hat_series = data_update.apply(predict, y_space = outcomeSpace[class_var], table = naive_tables, prior = prior_prob, axis = 1)\n",
    "    correct_predict = np.sum(data[class_var] == y_hat_series)\n",
    "    acc = correct_predict/len( y_hat_series)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7926"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "G_naive = learn_naive_bayes_structure(outcomeSpace,class_var)\n",
    "naive_tables = learn_bayes_net(G_naive, data, outcomeSpace)\n",
    "acc = assess_naive_bayes(G_naive, naive_tables, data, outcomeSpace, 'BC')\n",
    "acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a new function ``cv_naive_bayes(data, class_var)`` that uses ``assess_naive_bayes`` to assess the performance of the Na√Øve Bayes classifier in a dataset ``data``. To develop this code, perform the following steps:\n",
    "\n",
    "1. Use 10-fold cross-validation to split the data into training and test sets.\n",
    "\n",
    "2. Implement a function ``learn_naive_bayes_structure(outcomeSpace, class_var)`` to create and return a Na√Øve Bayes graph structure from ``outcomeSpace`` and ``class_var``. \n",
    "\n",
    "3. Use ``learn_bayes_net(G, data, outcomeSpace)`` to learn the Na√Øve Bayes parameters from a training set ``data``. \n",
    "\n",
    "4. Use ``assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var)`` to compute the accuracy of the Na√Øve Bayes classifier in a test set ``data``. Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy.\n",
    "\n",
    "Do 10-fold cross-validation, same as above, and return ``acc`` and ``stddev``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_naive_bayes(data, class_var) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_naive_bayes(data,class_var):\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    fold_len = int(data.shape[0]/10)\n",
    "    acc_list = []\n",
    "    G_naive = learn_naive_bayes_structure(outcomeSpace,class_var)\n",
    "    for i in range(10):\n",
    "        training_index = list(range(0,i*fold_len)) + list(range((i+1)*fold_len,data.shape[0]))\n",
    "        test_index = list(range(i*fold_len,(i+1)*fold_len))\n",
    "        training_data = data.iloc[training_index]\n",
    "        test_data = data.iloc[test_index]\n",
    "        naive_tables = learn_bayes_net(G_naive, training_data, outcomeSpace)\n",
    "        acc_list.append(assess_naive_bayes(G_naive, naive_tables, test_data, outcomeSpace,class_var))\n",
    "    return (np.mean(acc_list),np.std(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time consumption is 9.26 seconds\n",
      "The accuracy is 0.7919, and the standard deviation is 0.01\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "acc, stdev = cv_naive_bayes(data,class_var)\n",
    "end = time.time()\n",
    "print(\"The time consumption is {0:.2f} seconds\".format(end-begin))\n",
    "print(\"The accuracy is {0:.4f}, and the standard deviation is {1:.2f}\".format(acc,stddev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 5 - Tree-augmented Na√Øve Bayes Classification\n",
    "\n",
    "Similarly to the previous task, implement a Tree-augmented Na√Øve Bayes (TAN) classifier and evaluate your implementation in the breast cancer dataset. Design a function ``learn_tan_structure(data, outcomeSpace, class_var)`` to learn the TAN structure (graph) from the ``data`` and returns such a structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note by Yangqi : The following functions are used to enable more efficient (maybe) computation for mutual information \n",
    "\n",
    "To calculate conditional mutual info: \n",
    "https://en.wikipedia.org/wiki/Conditional_mutual_information#Some_identities\n",
    "\n",
    "\n",
    "$I(X;Y|Z) = H(X|Z) + H(Y|Z) - H(X,Y|Z)$\n",
    "\n",
    "$\\mathrm{H}(Y \\mid X)=-\\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} p(x, y) \\log \\frac{p(x, y)}{p(x)}$\n",
    "\n",
    "using the Entropy identity: we can calculate it easily. To calculate entropy, we need to know the distribution for each jotint var, \n",
    "df.value_counts will be usful \n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.value_counts.html\n",
    "\n",
    "Then we simply calculate entropy and workout the conditional mutual info. \n",
    "\n",
    "https://stackoverflow.com/questions/49685591/how-to-find-the-entropy-of-each-column-of-data-set-by-python\n",
    "\n",
    "After that, Bayesian graph can be quickly obtain by max span tree \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Calculate the conditional entropy\n",
    "def cond_entro(var_list,class_var,data):\n",
    "    probs_class_var = dict(data[class_var].value_counts(normalize=True,sort=False))\n",
    "    var_list.append(class_var)\n",
    "    probs_join_var = dict(data.groupby(var_list).size())\n",
    "    join_sum = sum(probs_join_var.values())\n",
    "    probs_join_var = {key:value/join_sum for key,value in probs_join_var.items()}\n",
    "    entropy = 0 \n",
    "    for key,value in probs_join_var.items():\n",
    "        entropy -= value * np.log(value/probs_class_var[key[-1]])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mutual information between var1 and var2 given class_var by using conditional entropy\n",
    "def MI(var1,var2,class_var,data):\n",
    "    mutual_info = cond_entro([var1],class_var,data) + \\\n",
    "                  cond_entro([var2],class_var,data) - \\\n",
    "                  cond_entro([var1,var2],class_var,data)\n",
    "    return(mutual_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following two functions are for checking whehter a graph is cyclic\n",
    "def isCyclicUtil(v,visited,parent,MST):\n",
    "    visited[v] = True \n",
    "    for node in MST[v]:\n",
    "        if (visited[node]==False):\n",
    "            if(isCyclicUtil(node,visited,v,MST)):\n",
    "                return True\n",
    "        elif(parent!=node):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def isCyclic(MST,node_list):\n",
    "    visited = {node:False for node in node_list}\n",
    "    \n",
    "    for node in node_list:\n",
    "        if (visited[node]==False):\n",
    "            if(isCyclicUtil(node,visited,None,MST)==True):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Return a maximum spanning tree for given edges\n",
    "def max_spanning_tree(input_edges,outcomeSpace,class_var):\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    node_list.remove(\"Metastasis\")\n",
    "    node_list.remove(\"LymphNodes\")\n",
    "    MST = {node:[] for node in node_list}\n",
    "    node_list.remove(class_var)\n",
    "    edge_count = 0\n",
    "    edges = copy.deepcopy(input_edges)\n",
    "    edges.sort(reverse=True,key=lambda x:x[1])\n",
    "    \n",
    "    while(len(edges)>0):\n",
    "        edge = edges.pop(0)[0]\n",
    "        MST[edge[0]].append(edge[1])\n",
    "        MST[edge[1]].append(edge[0])\n",
    "        edge_count +=1\n",
    "        \n",
    "        if (isCyclic(MST,node_list)):\n",
    "            MST[edge[0]].remove(edge[1])\n",
    "            MST[edge[1]].remove(edge[0])\n",
    "            edge_count -=1\n",
    "        \n",
    "        if (edge_count == len(node_list)-1):\n",
    "            return (MST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Maximum Spanning Tree with directed edges\n",
    "def generate_directed_MST(G_tan,MST,node,node_count):\n",
    "    children = MST[node]\n",
    "    for child in children:\n",
    "        MST[child].remove(node)\n",
    "        G_tan,node_count = generate_directed_MST(G_tan,MST,child,node_count)\n",
    "        G_tan[node].append(child) \n",
    "    node_count+=1\n",
    "    return (G_tan,node_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a graph that is in TAN structure\n",
    "def learn_tan_structure(data,outcomeSpace,class_var):\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    node_list.remove(\"Metastasis\")\n",
    "    node_list.remove(\"LymphNodes\")\n",
    "    edges = []\n",
    "    G_tan = {node:[] for node in node_list}\n",
    "    node_list.remove(class_var)\n",
    "    for i in range(len(node_list)-1):\n",
    "        for j in range(i+1,len(node_list)):\n",
    "            mutual_info = MI(node_list[i],node_list[j],class_var,data)\n",
    "            edges.append(((node_list[i],node_list[j]),mutual_info))\n",
    "    MST = max_spanning_tree(edges,outcomeSpace,class_var)\n",
    "    node_count = 0\n",
    "    for node in node_list:\n",
    "        if (node_count == len(node_list)):\n",
    "            break\n",
    "        G_tan,node_count = generate_directed_MST(G_tan,MST,node,node_count)\n",
    "    G_tan[class_var] = node_list\n",
    "    return(G_tan)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time consumption is 1.47 seconds\n",
      "Passed test case\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "begin = time.time()\n",
    "tan_graph = learn_tan_structure(data, outcomeSpace, class_var)\n",
    "end = time.time()\n",
    "print(\"The time consumption is {0:.2f} seconds\".format(end-begin))\n",
    "test(len(tan_graph['BC']) == len(tan_graph)-1)\n",
    "test('FibrTissueDev' in tan_graph['Spiculation'] or 'Spiculation' in tan_graph['FibrTissueDev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the other tasks, design a function ``cv_tan(data, class_var)`` that uses 10-fold cross-validation to assess the performance of the TAN classifier from ``data``. Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy. This function should use the ``learn_tan_structure`` as well as other functions defined in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_tan(data, class_var) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a single line of data, predict the classification of the class_var\n",
    "def tan_prediction(tan_tables, single_data_dict,outcomeSpace,class_var):\n",
    "    prior = tan_tables[class_var]['table']\n",
    "    prob = {name : np.log(prior[tuple([name])]) for name in outcomeSpace[class_var]}\n",
    "    for key,value in tan_tables.items():\n",
    "        if (key==class_var):\n",
    "            continue\n",
    "        for name in outcomeSpace[class_var]:\n",
    "            evidence = []\n",
    "            for i in range(len(value['dom'])):\n",
    "                if (class_var == value['dom'][i]):\n",
    "                    evidence.append(name) \n",
    "                else:\n",
    "                    evidence.append(single_data_dict[value['dom'][i]])\n",
    "            evidence = tuple(evidence)\n",
    "            prob[name] += np.log(value['table'][evidence])\n",
    "    return (max(prob, key=prob.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the accuracy of Tree-augmented Bayes Classifier on the test data\n",
    "def assess_tan(G_tan,tan_tables,test_data,outcomeSpace,class_var):\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    var_remove = ['Metastasis', 'LymphNodes',class_var]\n",
    "    var_list = [i for i in node_list if i not in var_remove] #now we get all the variables \n",
    "    test_data_update = test_data[var_list]\n",
    "    test_data_dict = test_data_update.to_dict(orient='records')\n",
    "    outcomeSpace_copy = { var: outcomeSpace[var] for var in var_list}\n",
    "    match_count = 0\n",
    "    for i in range(len(test_data_dict)):\n",
    "        pred = tan_prediction(tan_tables,test_data_dict[i],outcomeSpace,class_var)\n",
    "        if (pred == test_data.iloc[i][class_var]):\n",
    "            match_count +=1\n",
    "    return (match_count/test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_tan(data,class_var):\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    fold_len = int(data.shape[0]/10)\n",
    "    acc_list = []\n",
    "    for i in range(10):\n",
    "        training_index = list(range(0,i*fold_len)) + list(range((i+1)*fold_len,data.shape[0]))\n",
    "        test_index = list(range(i*fold_len,(i+1)*fold_len))\n",
    "        training_data = data.iloc[training_index]\n",
    "        test_data = data.iloc[test_index]\n",
    "        G_tan = learn_tan_structure(training_data,outcomeSpace,class_var)\n",
    "        tan_tables = learn_bayes_net(G_tan, training_data, outcomeSpace)\n",
    "        acc_list.append(assess_tan(G_tan, tan_tables, test_data, outcomeSpace,class_var))\n",
    "    return (np.mean(acc_list),np.std(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time consumption is 19.80 seconds\n",
      "The accuracy is 0.8329, and the standard deviation is 0.01\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "acc, stdev = cv_tan(data,class_var)\n",
    "end = time.time()\n",
    "print(\"The time consumption is {0:.2f} seconds\".format(end-begin))\n",
    "print(\"The accuracy is {0:.4f}, and the standard deviation is {1:.2f}\".format(acc,stddev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 6 - Report\n",
    "\n",
    "Write a report (**with less than 500 words**) summarising your findings in this assignment. Your report should address the following:\n",
    "\n",
    "a. Make a summary and discussion of the experimental results (accuracy). Use plots to illustrate your results.\n",
    "\n",
    "b. Discuss the complexity of the implemented algorithms.\n",
    "\n",
    "Use Markdown and Latex to write your report in the Jupyter notebook. Develop some plots using Matplotlib to illustrate your results. Be mindful of the maximum number of words. Please, be concise and objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "## Intro \n",
    "\n",
    "In this report, we investigate the prediction of breast cancer using various Bayesian network models. Our results suggest that Tree-augument Naive Bayes network is preferred compared to Bayesian netowrk classification and Naive Bayes method, considering the serious effect of false negatives, although the accuracy is slighly lower than the Bayesian classification network.  \n",
    "\n",
    "## Three methods \n",
    "\n",
    "### 1. Bayesian classificantion network \n",
    "\n",
    "Bayesian classification network is built according to an opnion that relevant features will influence the likelihood of breast cancer. In this model, the variables's casual relationships are derived from medical research theories. This will lead to an automatic variable selection process by Markov blanket, with given evidence. For example, in our netwrok G, variables such as *Size*, *Shape* and *Margin* are removed from this classfication model. This method is preferred when the causal relationship between variables are well-established or when we are having too many variables in the model that building a Tree-augmented Bayes Network on them would be time-consuming.\n",
    "\n",
    "\n",
    "### 2. Naive Bayes \n",
    "\n",
    "Naive Bayes assumes all the features are indepedent so the joint distribution of $p(y,x_1,\\cdots, x_m) = p(y) \\Pi_i p(x_i|y)$ becomes a mutliplication of m conditional distribution with the prior. This significantly reduce the number of parameters from exponential to linear. However, the indepedent assumption could be questionable in the model.\n",
    "\n",
    "### 3. Tree-augmented Bayes network \n",
    "\n",
    "Tree-augument Bayes network (TAN) allows the dependency between features by calculating the conditional mutual information between each pair of variables and establish the dependency relationship according to maximum spanning tree algorithm. Since only one parent is allowed amongst features, the parametrs space is growing from linear to quadratic level, which enables efficient computation without indepdent assumption.\n",
    "\n",
    "\n",
    "\n",
    "## Results and discussion \n",
    "\n",
    "The classifcation variable BC has three categories$\\{c_1, c_2, c_3\\}= $ {'Insitu', 'Invasive', 'No'}. After develop the models, the inquiry is $\\hat{c} = \\arg_{c_i} \\max p(c_i, e)$ and the accuracy is calculated by $acc = \\frac{\\sum I(\\hat{c_i} = c_i)}{N} $ The results and the computation time with 10-fold validation is given in figure 1. \n",
    "\n",
    "\n",
    "According to the results, it is obvious that Bayes network classifcation is outperforming the other two models with much greater accuracy but also lower computation time.\n",
    " \n",
    "\n",
    "\n",
    "Another important aspect of detection of breast cancer is about assignning different weights to false negative and false positive. Since there are three categories of BC, the accuracy in prediction given the true value in each categories are reported.\n",
    "\n",
    "Bayes networks \n",
    "\n",
    "Naive Bayes \n",
    "\n",
    "TAN\n",
    "\n",
    "False negative is a much more serious issue than False positive in medical detection. In this case, the false negative includes (BC =Insitu, prediction = No),  (BC =Invasive, prediction = No); as they will lead to no treatment for those patient.\n",
    "According to the table above, we can see some advantages of TAN compared to the other two model. \n",
    "For patient with Insitu, TAN classified 14% of them as No and for patient with Invasive, TAN calssifies 7 % of them as No, both of which are lower than the other two models\n",
    "Therefore, despite of slighly lower total accuracy of TAN, it is still preferred when we consider the serious outcome of false negative.\n",
    "\n",
    "\n",
    "## Complexity and computation time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = {'Accuracy': [0.8412,0.7919,0.8329], 'Time': [6.76, 9.26,19.80]}\n",
    "df = pd.DataFrame.from_dict(results)\n",
    "df.index = ['Bayes net', \"Naive Bayes\", \"TAN\"]\n",
    "\n",
    "\n",
    "fig = plt.figure() # Create matplotlib figure\n",
    "\n",
    "ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.\n",
    "\n",
    "width = 0.25\n",
    "\n",
    "df.Accuracy.plot(kind='bar', color='red', ax=ax, width=width, position=1)\n",
    "df.Time.plot(kind='bar', color='blue', ax=ax2, width=width, position=0)\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax2.set_ylabel('Computation (sec)')\n",
    "\n",
    "ax.set_ylim([0,1])\n",
    "ax2.set_ylim([0,20])\n",
    "\n",
    "plt.title('Model comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_bayes_net2(G, prob_tables, data, outcomeSpace, class_var):\n",
    "    \n",
    "    var_blanket = markov_blanket(G,class_var)\n",
    "    blanket_without_var = copy.deepcopy(var_blanket)\n",
    "    var_blanket.append(class_var)\n",
    "    var_remove = ['Metastasis', 'LymphNodes']\n",
    "    var_list = [i for i in var_blanket if i not in var_remove] # now we get the variables that needs for inference class_var \n",
    "    \n",
    "#     begin = time.time()\n",
    "    p_table =  p_joint_blanket(var_list, outcomeSpace, prob_tables)\n",
    "#     end = time.time()\n",
    "#     print(\"The joining table costs {0:.2f} seconds\".format(end-begin))\n",
    "    \n",
    "    \n",
    "    \n",
    "    q_var = class_var\n",
    "    evidence_list = [var for var in var_list if var!=class_var]\n",
    "    data_update = data[evidence_list]\n",
    "    \n",
    "    data_dict = data_update.to_dict(orient='records')\n",
    "    outcomeSpace_copy = { var: outcomeSpace[var] for var in var_list}\n",
    "    match_count = 0\n",
    "    pred_list = []\n",
    "    for i in range(len(data_dict)):\n",
    "        q_table = query(p_table, outcomeSpace_copy, q_var, **data_dict[i])\n",
    "        pred = max(q_table['table'],key=q_table['table'].get)[0]\n",
    "        pred_list.append(pred)\n",
    "    return (pred_list)\n",
    "\n",
    "def assess_naive_bayes2(G_naive, naive_tables, data, outcomeSpace, class_var):\n",
    "    naive_tables = additive_smoothing(naive_tables,data)\n",
    "    \n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    var_remove = ['Metastasis', 'LymphNodes']\n",
    "    var_list = [i for i in node_list if i not in var_remove] #now we get all the variables \n",
    "    \n",
    "    evidence_list = [var for var in var_list if var!=class_var]\n",
    "    data_update = data[evidence_list]\n",
    "    \n",
    "    prior_prob = data[class_var].value_counts(normalize = True)   \n",
    "    y_hat_series = data_update.apply(predict, y_space = outcomeSpace[class_var], table = naive_tables, prior = prior_prob, axis = 1)\n",
    "    #correct_predict = np.sum(data[class_var] == y_hat_series)\n",
    "    #acc = correct_predict/len( y_hat_series)\n",
    "    \n",
    "    return  y_hat_series\n",
    "\n",
    "def assess_tan2(G_tan,tan_tables,test_data,outcomeSpace,class_var):\n",
    "    tan_tables = update_tan_tables(tan_tables)\n",
    "    prior = tan_tables[class_var]['table'][()]\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    var_remove = ['Metastasis', 'LymphNodes',class_var]\n",
    "    var_list = [i for i in node_list if i not in var_remove] #now we get all the variables \n",
    "    data_update = data[var_list]\n",
    "    data_dict = data_update.to_dict(orient='records')\n",
    "    outcomeSpace_copy = { var: outcomeSpace[var] for var in var_list}\n",
    "    match_count = 0\n",
    "    pred_list = []\n",
    "    for i in range(len(data_dict)):\n",
    "        pred = tan_prediction(tan_tables,data_dict[i],outcomeSpace,class_var)\n",
    "        pred_list.append(pred)\n",
    "        if (pred == data.iloc[i][class_var]):\n",
    "            match_count +=1\n",
    "    return (pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_bayes =  np.array(assess_bayes_net2(G, prob_tables, data, outcomeSpace, class_var))\n",
    "data['yhat_bayes'] = yhat_bayes \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_naive = assess_naive_bayes2(G_naive, naive_tables, data, outcomeSpace, 'BC')\n",
    "data['yhat_naive'] = yhat_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_tan = learn_tan_structure(data,outcomeSpace,class_var)\n",
    "tan_tables = learn_bayes_net(G_tan,data, outcomeSpace)\n",
    "yhat_tan = assess_tan2(G_tan,tan_tables,data,outcomeSpace,class_var)\n",
    "data['yhat_tan'] = yhat_tan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prediction</th>\n",
       "      <th>Insitu</th>\n",
       "      <th>Invasive</th>\n",
       "      <th>No</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Insitu</th>\n",
       "      <td>0.338971</td>\n",
       "      <td>0.421071</td>\n",
       "      <td>0.239958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Invasive</th>\n",
       "      <td>0.106924</td>\n",
       "      <td>0.789329</td>\n",
       "      <td>0.103748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.014390</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.977249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.251350</td>\n",
       "      <td>0.666350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prediction    Insitu  Invasive        No\n",
       "True                                    \n",
       "Insitu      0.338971  0.421071  0.239958\n",
       "Invasive    0.106924  0.789329  0.103748\n",
       "No          0.014390  0.008361  0.977249\n",
       "All         0.082300  0.251350  0.666350"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(data['BC'],data['yhat_bayes'], rownames = ['True'],colnames = ['Prediction'], margins = True, normalize = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prediction</th>\n",
       "      <th>Insitu</th>\n",
       "      <th>Invasive</th>\n",
       "      <th>No</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Insitu</th>\n",
       "      <td>0.392882</td>\n",
       "      <td>0.396054</td>\n",
       "      <td>0.211064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Invasive</th>\n",
       "      <td>0.146729</td>\n",
       "      <td>0.723904</td>\n",
       "      <td>0.129367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.083930</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>0.909880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>0.142600</td>\n",
       "      <td>0.231000</td>\n",
       "      <td>0.626400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prediction    Insitu  Invasive        No\n",
       "True                                    \n",
       "Insitu      0.392882  0.396054  0.211064\n",
       "Invasive    0.146729  0.723904  0.129367\n",
       "No          0.083930  0.006190  0.909880\n",
       "All         0.142600  0.231000  0.626400"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(data['BC'],data['yhat_naive'] , rownames = ['True'],colnames = ['Prediction'], margins = True, normalize = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prediction</th>\n",
       "      <th>Insitu</th>\n",
       "      <th>Invasive</th>\n",
       "      <th>No</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Insitu</th>\n",
       "      <td>0.555673</td>\n",
       "      <td>0.299859</td>\n",
       "      <td>0.144468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Invasive</th>\n",
       "      <td>0.227610</td>\n",
       "      <td>0.698920</td>\n",
       "      <td>0.073470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.072835</td>\n",
       "      <td>0.006914</td>\n",
       "      <td>0.920251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>0.177900</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.610200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prediction    Insitu  Invasive        No\n",
       "True                                    \n",
       "Insitu      0.555673  0.299859  0.144468\n",
       "Invasive    0.227610  0.698920  0.073470\n",
       "No          0.072835  0.006914  0.920251\n",
       "All         0.177900  0.211900  0.610200"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(data['BC'],data['yhat_tan'] , rownames = ['True'],colnames = ['Prediction'], margins = True, normalize = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your report in one or more cells here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
