{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9418 - Assignment 1 - Bayesian Networks as Classifiers\n",
    "\n",
    "## UNSW Sydney, October 2020\n",
    "\n",
    "- Minrui Lu z5277884\n",
    "- Yangqi Zhang z5235062"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "**Submission deadline:** Sunday, 18th October 2020, at 18:00:00.\n",
    "\n",
    "**Late Submission Policy:** The penalty is set at 20% per late day. This is ceiling penalty, so if a group is marked 60/100 and they submitted two days late, they still get 60/100.\n",
    "\n",
    "**Form of Submission:** This is a group assignment. Each group can have up to **two** students. **Only one member of the group should submit the assignment**.\n",
    "\n",
    "You can reuse any piece of source code developed in the tutorials.\n",
    "\n",
    "Submit your files using give. On a CSE Linux machine, type the following on the command-line:\n",
    "\n",
    "``$ give cs9418 ass1 solution.zip``\n",
    "\n",
    "Alternative, you can submit your solution via the [WebCMS](https://webcms3.cse.unsw.edu.au/COMP9418/20T3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical prerequisites\n",
    "\n",
    "These are the libraries your are allowed to use. No other libraries will be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make division default to floating-point, saving confusion\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Allowed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import heapq as pq\n",
    "import matplotlib as mp\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "from collections import OrderedDict as odict\n",
    "from graphviz import Digraph\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Supplemental libraries\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial task - Initialise graph\n",
    "\n",
    "Create a graph ``G`` that represents the following network by filling in the edge lists.\n",
    "![Bayes Net](BayesNet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = {\n",
    "    \"BreastDensity\" : [\"Mass\"],\n",
    "    \"Location\" : [\"BC\"],\n",
    "    \"Age\" : [\"BC\"],\n",
    "    \"BC\" : [\"Mass\", \"AD\", \"Metastasis\", \"MC\", \"SkinRetract\",\"NippleDischarge\"],\n",
    "    \"Mass\" : [\"Size\",  \"Shape\", \"Margin\" ],\n",
    "    \"AD\" : [\"FibrTissueDev\"],\n",
    "    \"Metastasis\" : [ \"LymphNodes\"],\n",
    "    \"MC\" : [],\n",
    "    \"Size\" : [],\n",
    "    \"Shape\" : [],\n",
    "    \"FibrTissueDev\" : [ \"SkinRetract\" , \"NippleDischarge\",\"Spiculation\" ],\n",
    "    \"LymphNodes\" : [],\n",
    "    \"SkinRetract\" : [],\n",
    "    \"NippleDischarge\" : [],\n",
    "    \"Spiculation\" : [\"Margin\" ],\n",
    "    \"Margin\" : [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 1 - Efficient d-separation test\n",
    "\n",
    "Implement the efficient version of the d-separation algorithm in a function ``d_separation(G, X, Z, Y)`` that return a boolean: true if **X** is d-separated from **Y** given **Z** in the graph $G$ and false otherwise.\n",
    "\n",
    "* **X**,**Y** and **Z** are python sets, each containing a set of variable names. \n",
    "* Variable names may be strings or integers, and can be assumed to be nodes of the graph $G$. \n",
    "* $G$ is a graph as defined in tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "def isleaf_node(G,node):\n",
    "    return not G[node] \n",
    "\n",
    "# delete the leaf node and its edges from the G, return a new Graph\n",
    "def remove_leaf(G1,leaf_node):\n",
    "    G_new = copy.deepcopy(G1)\n",
    "    del G_new[leaf_node]\n",
    "    for key, value in  G_new.items(): \n",
    "        if leaf_node in value:\n",
    "            G_new[key].remove(leaf_node)\n",
    "    return G_new\n",
    "    \n",
    "def repeat_del(G1,node_list): \n",
    "    count = 1 \n",
    "    list_update = node_list.copy()\n",
    "    while count > 0:\n",
    "        count = 0\n",
    "        for node in node_list: \n",
    "            if isleaf_node(G1,node):\n",
    "                #remove the nodes and update the graph \n",
    "                G1= copy.deepcopy(remove_leaf(G1,node))\n",
    "                list_update.remove(node)\n",
    "                count = count + 1 \n",
    "        node_list = list_update.copy()\n",
    "    return (G1)\n",
    "\n",
    "    \n",
    "def dfs_r(G, v, colour):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `G`, an adjacency list representation of a graph\n",
    "    `v`, next vertex to be visited\n",
    "    `colour`, dictionary with the colour of each node\n",
    "    \"\"\"\n",
    "    # Visited vertices are coloured 'grey'\n",
    "    colour[v] = 'grey'\n",
    "    # Let's visit all outgoing edges from v\n",
    "    for w in G[v]:\n",
    "        # To avoid loops, we vist check if the next vertex hasn't been visited yet\n",
    "        if colour[w] == 'white':\n",
    "            dfs_r(G, w, colour)\n",
    "    # When we finish the for loop, we know we have visited all nodes from v. It is time to turn it 'black'\n",
    "    colour[v] = 'black' \n",
    "    return None\n",
    "  \n",
    "\n",
    "\n",
    "def d_separation(G1, X, Z, Y): \n",
    "    \"\"\" \n",
    "    Arguments: \n",
    "    `G`, an adjacency list representation of a graph \n",
    "    `X`, a set of variables name \n",
    "    `Y`, a set of variables name \n",
    "    `Z`, a set of a set of variables name \n",
    "    \n",
    "    Returns \n",
    "    a boolean: true if X is d-separated from Y given Z in the graph  ùê∫  and false otherwise.\n",
    "    \n",
    "    \"\"\"\n",
    "    if bool(X.intersection(Y).intersection(Z)):\n",
    "        raise Exception(\"X, Y, Z are not disjoint\") \n",
    "    \n",
    "    combine_set = X.union(Y).union(Z)\n",
    "    node_set = set(G1.keys())\n",
    "    remain_nodes = set(node_set  - combine_set)\n",
    "    \n",
    "    G_final = copy.deepcopy(repeat_del(G1,remain_nodes)) \n",
    "    \n",
    "    for var in Z: \n",
    "        G_final[var] = [] \n",
    "    \n",
    "    for key,values in G_final.items():\n",
    "        if bool(values):\n",
    "            for n in values: \n",
    "                G_final[n].append(key) \n",
    "\n",
    "    colour = {node: 'white' for node in G_final.keys()}\n",
    "    #check connectivity \n",
    "    separate = True \n",
    "    for nodex in X:\n",
    "        dfs_r(G_final,nodex,colour) \n",
    "        Y_color = [colour[node] for node in Y]\n",
    "        if 'black' in Y_color:\n",
    "            separate = False\n",
    "    return(separate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "def test(statement):\n",
    "    if statement:\n",
    "        print(\"Passed test case\")\n",
    "    else:\n",
    "        print(\"Failed test case\")\n",
    "        \n",
    "test(d_separation(G, set(['Age']), set(['BC']), set(['AD'])))\n",
    "test(not d_separation(G, set(['Spiculation','LymphNodes']), set(['MC', 'Size']), set(['Age'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Marks] Task 2 - Estimate Bayesian Network parameters from data\n",
    "\n",
    "Implement a function ``learn_outcome_space(data)`` that learns the outcome space (the valid values for each variable) from the pandas dataframe ``data`` and returns a dictionary ``outcomeSpace`` with these values.\n",
    "\n",
    "Implement a function ``learn_bayes_net(G, data, outcomeSpace)`` that learns the parameters of the Bayesian Network $G$. This function should return a dictionary ``prob_tables`` with the all conditional probability tables (one for each node).\n",
    "\n",
    "- ``G`` is a directed acyclic graph. For this part of the assignment, $G$ should be declared according to the breast cancer Bayesian network presented in the diagram in the assignment specification.\n",
    "- ``data`` is a dataframe created from a csv file containing the relevant data. \n",
    "- ``outcomeSpace`` is defined in tutorials.\n",
    "- ``prob_tables`` is a dict from each variable name (node) to a \"factor\". Factors are defined in tutorial 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_outcome_space(data) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_outcome_space(data):\n",
    "    outcomeSpace = {}\n",
    "    for attr in data.columns:\n",
    "        outcomeSpace[attr] = tuple(data[attr].unique())\n",
    "    return(outcomeSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "with open('bc.csv') as file:\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "outcomeSpace = learn_outcome_space(data)\n",
    "\n",
    "outcomes = outcomeSpace['BreastDensity']\n",
    "answer = ('high', 'medium', 'low')\n",
    "test(len(outcomes) == len(answer) and set(outcomes) == set(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_bayes_net(G, data, outcomeSpace) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxilliary functions\n",
    "def printFactor(f):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, a factor to print on screen\n",
    "    \"\"\"\n",
    "    # Create a empty list that we will fill in with the probability table entries\n",
    "    table = list()\n",
    "    \n",
    "    # Iterate over all keys and probability values in the table\n",
    "    for key, item in f['table'].items():\n",
    "        k = list(key)\n",
    "        k.append(item)\n",
    "        table.append(k)\n",
    "    dom = list(f['dom'])\n",
    "    dom.append('Pr')\n",
    "    print(tabulate(table,headers=dom,tablefmt='fancy_grid'))\n",
    "    \n",
    "def prob(factor, *entry):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `factor`, a dictionary of domain and probability values,\n",
    "    `entry`, a list of values, one for each variable in the same order as specified in the factor domain.\n",
    "    \n",
    "    Returns p(entry)\n",
    "    \"\"\"\n",
    "    return factor['table'][entry]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allEqualThisIndex(dict_of_arrays,**fixed_vars):\n",
    "    first_array = dict_of_arrays[list(dict_of_arrays.keys())[0]]\n",
    "    index = np.ones_like(first_array,dtype=np.bool_)\n",
    "    for var_name,var_val in fixed_vars.items():\n",
    "        index = index & (np.asarray(dict_of_arrays[var_name])==var_val)\n",
    "    return (index)\n",
    "\n",
    "def estProbTable(data,var_name,parent_names,outcomeSpace,alpha=1):\n",
    "    var_outcomes = outcomeSpace[var_name]\n",
    "    parent_outcomes = [outcomeSpace[var] for var in parent_names]\n",
    "    all_parent_combinations = product(*parent_outcomes)\n",
    "    prob_table = odict()\n",
    "    \n",
    "    for i,parent_combination in enumerate(all_parent_combinations):\n",
    "        parent_vars = dict(zip(parent_names,parent_combination))\n",
    "        parent_index = allEqualThisIndex(data,**parent_vars)\n",
    "        for var_outcome in var_outcomes:\n",
    "            var_index = (np.asarray(data[var_name])==var_outcome)\n",
    "            new_dom = tuple(list(parent_combination)+[var_outcome])\n",
    "\n",
    "            # Additive smoothing is applied here\n",
    "            \n",
    "            N = parent_index.sum()\n",
    "            c = (var_index & parent_index).sum()\n",
    "            X_cardinality = len(var_outcomes)\n",
    "            prob_table[new_dom] = (c+alpha)/(N+alpha*X_cardinality)\n",
    "            \n",
    "    return({'dom':tuple(list(parent_names)+[var_name]),'table':prob_table})\n",
    "\n",
    "def transposeGraph(G):\n",
    "    GT = dict((v,[]) for v in G)\n",
    "    for v in G:\n",
    "        for w in G[v]:\n",
    "            GT[w].append(v)\n",
    "    return (GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_bayes_net(G,data,outcomeSpace):\n",
    "    bayes_net = odict()\n",
    "    GT = transposeGraph(G)\n",
    "    for child, parents in GT.items():\n",
    "        bayes_net[child] = estProbTable(data,child,parents,outcomeSpace)\n",
    "    return(bayes_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "prob_tables = learn_bayes_net(G, data, outcomeSpace)\n",
    "test(abs(prob_tables['Age']['table'][('35-49',)] - 0.2476) < 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 3 - Bayesian Network Classification\n",
    "\n",
    "Design a new function ``assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)`` that uses the test cases in ``data`` to assess the performance of the Bayesian network defined by ``G`` and ``prob_tables``. Implement the efficient classification procedure discussed in the lectures. Such a function should return the classifier accuracy. \n",
    " * ``class_var`` is the name of the variable you are predicting, using all other variables.\n",
    " * ``outcomeSpace`` was created in task 2\n",
    " \n",
    "Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy.\n",
    "\n",
    "Return just the accuracy:\n",
    "\n",
    "``acc = assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var) in one or more cells here\n",
    "def join(f1, f2, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f1`, first factor to be joined.\n",
    "    `f2`, second factor to be joined.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor with a join of f1 and f2\n",
    "    \"\"\"\n",
    "    common_vars = list(f1['dom']) + list(set(f2['dom']) - set(f1['dom']))\n",
    "    table = list()\n",
    "    for entries in product(*[outcomeSpace[node] for node in common_vars]):\n",
    "        entryDict = dict(zip(common_vars, entries))\n",
    "        f1_entry = (entryDict[var] for var in f1['dom'])\n",
    "        f2_entry = (entryDict[var] for var in f2['dom'])\n",
    "        p1 = prob(f1, *f1_entry)          \n",
    "        p2 = prob(f2, *f2_entry)           \n",
    "        table.append((entries, p1 * p2))\n",
    "    return {'dom': tuple(common_vars), 'table': odict(table)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a markov blanket of a given variable in the graph\n",
    "def markov_blanket(G,var):\n",
    "    blanket_list =  G[var] #include the children\n",
    "    children_list = blanket_list \n",
    "    GT = transposeGraph(G)\n",
    "    blanket_list = blanket_list + GT[var] #include the parents     \n",
    "    for node in children_list:\n",
    "        blanket_list = blanket_list + GT[node] #include spouse \n",
    "    \n",
    "    blanket_list = list(set(blanket_list))\n",
    "    blanket_list = [i for i in blanket_list if i != var]\n",
    "    return blanket_list\n",
    "\n",
    "# Join each variable's markov blanket in the outcomeSpace together\n",
    "def p_joint_blanket(my_blanket, outcomeSpace, cond_tables):\n",
    "    var_list = my_blanket\n",
    "    p = join(cond_tables[var_list[0]], cond_tables[var_list[1]], outcomeSpace)\n",
    "    for var in var_list[2:]:\n",
    "        p = join(p,cond_tables[var], outcomeSpace)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidence(var, e, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `var`, a valid variable identifier.\n",
    "    `e`, the observed value for var.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns dictionary with a copy of outcomeSpace with var = e\n",
    "    \"\"\"    \n",
    "    newOutcomeSpace = outcomeSpace.copy()      \n",
    "    newOutcomeSpace[var] = (e,)               \n",
    "    return newOutcomeSpace\n",
    "\n",
    "def marginalize(f, var, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be marginalized.\n",
    "    `var`, variable to be summed out.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor f' with dom(f') = dom(f) - {var}\n",
    "    \"\"\"    \n",
    "    new_dom = list(f['dom'])\n",
    "    \n",
    "    new_dom.remove(var)           \n",
    "    table = list()                 \n",
    "    for entries in product(*[outcomeSpace[node] for node in new_dom]):\n",
    "        s = 0;                     \n",
    "        for val in outcomeSpace[var]:\n",
    "            entriesList = list(entries)\n",
    "            entriesList.insert(f['dom'].index(var), val)\n",
    "            p = prob(f, *tuple(entriesList))     \n",
    "            s = s + p                           \n",
    "        \n",
    "        table.append((entries, s))\n",
    "    return {'dom': tuple(new_dom), 'table': odict(table)}\n",
    "\n",
    "\n",
    "def normalize(f):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be normalized.\n",
    "    \n",
    "    Returns a new factor f' as a copy of f with entries that sum up to 1\n",
    "    \"\"\" \n",
    "    table = list()\n",
    "    sum = 0\n",
    "    for k, p in f['table'].items():\n",
    "        sum = sum + p\n",
    "    for k, p in f['table'].items():\n",
    "        table.append((k, p/sum))\n",
    "    return {'dom': f['dom'], 'table': odict(table)}\n",
    "\n",
    "\n",
    "def query(p, outcomeSpace, q_vars, **q_evi):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `p`, probability table to query.\n",
    "    `outcomeSpace`, dictionary will variable domains\n",
    "    `q_vars`, list of variables in query head\n",
    "    `q_evi`, dictionary of evidence in the form of variables names and values\n",
    "    \n",
    "    Returns a new factor NORMALIZED factor will all hidden variables eliminated as evidence set as in q_evi\n",
    "    \"\"\"     \n",
    "    pm = p.copy()\n",
    "    outSpace = outcomeSpace.copy()\n",
    "\n",
    "    for var_evi, e in q_evi.items():\n",
    "        outSpace = evidence(var_evi, e, outSpace)\n",
    "    for var in outSpace:\n",
    "        if not var in q_vars:\n",
    "            pm = marginalize(pm, var, outSpace)\n",
    "    return normalize(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the accuracy of a Bayes network given a data\n",
    "def assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var): \n",
    "    var_blanket = markov_blanket(G,class_var)\n",
    "    var_blanket.append(class_var)\n",
    "    var_remove = ['Metastasis', 'LymphNodes']\n",
    "    # now we get the variables that needs for inference class_var\n",
    "    var_list = [i for i in var_blanket if i not in var_remove]  \n",
    "    p_table =  p_joint_blanket(var_list, outcomeSpace, prob_tables) \n",
    "    q_var = class_var\n",
    "    evidence_list = [var for var in var_list if var!=class_var]\n",
    "    data_update = data[evidence_list]\n",
    "    data_dict = data_update.to_dict(orient='records')\n",
    "    new_outcomeSpace = { var: outcomeSpace[var] for var in var_list}\n",
    "    match_count = 0\n",
    "    for i in range(len(data_dict)):\n",
    "        q_table = query(p_table, new_outcomeSpace, q_var, **data_dict[i])\n",
    "        pred = max(q_table['table'],key=q_table['table'].get)[0]\n",
    "        if (pred == data.iloc[i][q_var]):\n",
    "            match_count +=1\n",
    "    return (match_count/data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "class_var = \"BC\"\n",
    "acc = assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8423"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a function ``cv_bayes_net(G, data, class_var)`` that uses ``learn_outcome_space``, ``learn_bayes_net``and ``assess_bayes_net`` to learn and assess a Bayesian network in a dataset using 10-fold cross-validation. Compute and report the average accuracy over the ten cross-validation runs as well as the standard deviation, e.g.\n",
    "\n",
    "``acc, stddev = cv_bayes_net(G, data, class_var)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_bayes_net(G, data, class_var) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross validation is 10 fold here\n",
    "def cv_bayes_net(G,data,class_var):\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    fold_len = int(data.shape[0]/10)\n",
    "    acc_list = []\n",
    "    for i in range(10):\n",
    "        training_index = list(range(0,i*fold_len)) + list(range((i+1)*fold_len,data.shape[0]))\n",
    "        test_index = list(range(i*fold_len,(i+1)*fold_len))\n",
    "        training_data = data.iloc[training_index]\n",
    "        test_data = data.iloc[test_index]\n",
    "        prob_tables = learn_bayes_net(G,training_data,outcomeSpace)\n",
    "        acc_list.append(assess_bayes_net(G,prob_tables,test_data,outcomeSpace,class_var))\n",
    "    return (np.mean(acc_list),np.std(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.8412, and the standard deviation is 0.01\n",
      "The time consumption is 6.57 seconds\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "begin = time.time()\n",
    "acc, stddev = cv_bayes_net(G, data, 'BC')\n",
    "end = time.time()\n",
    "print(\"The accuracy is {0:.4f}, and the standard deviation is {1:.2f}\".format(acc,stddev))\n",
    "print(\"The time consumption is {0:.2f} seconds\".format(end-begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Marks] Task 4 - Na√Øve Bayes Classification\n",
    "\n",
    "Design a new function ``assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var)`` to classify and assess the test cases in a dataset ``data`` according to the Na√Øve Bayes classifier. To classify each example, use the log probability trick discussed in the lectures. This function should return the accuracy of the classifier in ``data``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var) in one or more cells here\n",
    "\n",
    "# Return a Naive Bayes graph, in which there is only one parent, and other nodes are all its child\n",
    "def learn_naive_bayes_structure(outcomeSpace, class_var):\n",
    "    \"\"\"Return the naive-bayes graph structure (a dict) according to above info\"\"\"\n",
    "    G_nb = {}\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    node_list.remove(class_var)\n",
    "    G_nb[class_var] = node_list\n",
    "    for nodes in node_list:\n",
    "        G_nb[nodes] = []\n",
    "    return G_nb\n",
    "\n",
    "def additive_smoothing(prop_tables,data,alpha=1):\n",
    "    N = data.shape[0]\n",
    "    tables = copy.deepcopy(prop_tables)\n",
    "    for node , table_dict in tables.items():\n",
    "        X_cardinality = len(table_dict['table'].values())\n",
    "        if (0 in table_dict['table'].values()): # addtive smoothing required\n",
    "            for nameSpace, prob in table_dict['table'].items():\n",
    "                count_X_eq_x = int(round(prob * N,0))\n",
    "                table_dict['table'][nameSpace] = (count_X_eq_x+alpha)/(N+alpha*X_cardinality)\n",
    "    return (tables)\n",
    "\n",
    "#Return the log likelihood for each evidence variable\n",
    "def single_var_query( e, node_table):\n",
    "    # using log to avoid probability diminishing\n",
    "    prob_with_evi = {key[0]: np.log(value) for key,value in node_table['table'].items() if key[1] == e}    \n",
    "    return prob_with_evi\n",
    "\n",
    "# Given a line of data, return the class prediction\n",
    "def predict(x, y_space, table, prior):\n",
    "    # initialize the prediction dictionary by the prior probabilities\n",
    "    pre_dict = {i:  np.log(prior[i]) for i in y_space if prior[i]!=0}\n",
    "    for i in range(len(x)):\n",
    "        var_prob = single_var_query(x[i], table[x.index[i]])\n",
    "        for key in pre_dict.keys():\n",
    "            pre_dict[key] = pre_dict[key] +  var_prob[key]    \n",
    "    yhat = max(pre_dict, key=pre_dict.get)   \n",
    "    return yhat \n",
    "\n",
    "# Return the accuracy of a Naive Bayes network in the test data\n",
    "def assess_naive_bayes(G_naive, naive_tables, data, outcomeSpace, class_var):\n",
    "    naive_tables = additive_smoothing(naive_tables,data)   \n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    var_remove = ['Metastasis', 'LymphNodes']\n",
    "    var_list = [i for i in node_list if i not in var_remove] #now we get all the variables    \n",
    "    evidence_list = [var for var in var_list if var!=class_var]\n",
    "    data_update = data[evidence_list]\n",
    "    prior_prob = data[class_var].value_counts(normalize = True)   \n",
    "    y_hat_series = data_update.apply(predict, y_space = outcomeSpace[class_var], table = naive_tables, prior = prior_prob, axis = 1)\n",
    "    correct_predict = np.sum(data[class_var] == y_hat_series)\n",
    "    acc = correct_predict/len( y_hat_series)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7926"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "G_naive = learn_naive_bayes_structure(outcomeSpace,class_var)\n",
    "naive_tables = learn_bayes_net(G_naive, data, outcomeSpace)\n",
    "acc = assess_naive_bayes(G_naive, naive_tables, data, outcomeSpace, 'BC')\n",
    "acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a new function ``cv_naive_bayes(data, class_var)`` that uses ``assess_naive_bayes`` to assess the performance of the Na√Øve Bayes classifier in a dataset ``data``. To develop this code, perform the following steps:\n",
    "\n",
    "1. Use 10-fold cross-validation to split the data into training and test sets.\n",
    "\n",
    "2. Implement a function ``learn_naive_bayes_structure(outcomeSpace, class_var)`` to create and return a Na√Øve Bayes graph structure from ``outcomeSpace`` and ``class_var``. \n",
    "\n",
    "3. Use ``learn_bayes_net(G, data, outcomeSpace)`` to learn the Na√Øve Bayes parameters from a training set ``data``. \n",
    "\n",
    "4. Use ``assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var)`` to compute the accuracy of the Na√Øve Bayes classifier in a test set ``data``. Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy.\n",
    "\n",
    "Do 10-fold cross-validation, same as above, and return ``acc`` and ``stddev``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_naive_bayes(data, class_var) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_naive_bayes(data,class_var):\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    fold_len = int(data.shape[0]/10)\n",
    "    acc_list = []\n",
    "    G_naive = learn_naive_bayes_structure(outcomeSpace,class_var)\n",
    "    for i in range(10):\n",
    "        training_index = list(range(0,i*fold_len)) + list(range((i+1)*fold_len,data.shape[0]))\n",
    "        test_index = list(range(i*fold_len,(i+1)*fold_len))\n",
    "        training_data = data.iloc[training_index]\n",
    "        test_data = data.iloc[test_index]\n",
    "        naive_tables = learn_bayes_net(G_naive, training_data, outcomeSpace)\n",
    "        acc_list.append(assess_naive_bayes(G_naive, naive_tables, test_data, outcomeSpace,class_var))\n",
    "    return (np.mean(acc_list),np.std(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time consumption is 3.90 seconds\n",
      "The accuracy is 0.7919, and the standard deviation is 0.01\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "acc, stdev = cv_naive_bayes(data,class_var)\n",
    "end = time.time()\n",
    "print(\"The time consumption is {0:.2f} seconds\".format(end-begin))\n",
    "print(\"The accuracy is {0:.4f}, and the standard deviation is {1:.2f}\".format(acc,stddev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 5 - Tree-augmented Na√Øve Bayes Classification\n",
    "\n",
    "Similarly to the previous task, implement a Tree-augmented Na√Øve Bayes (TAN) classifier and evaluate your implementation in the breast cancer dataset. Design a function ``learn_tan_structure(data, outcomeSpace, class_var)`` to learn the TAN structure (graph) from the ``data`` and returns such a structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the formulas below, we can calculate the mutual information by computing conditional entropies.\n",
    "\n",
    "$I(X;Y|Z) = H(X|Z) + H(Y|Z) - H(X,Y|Z)$\n",
    "\n",
    "$\\mathrm{H}(Y \\mid X)=-\\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} p(x, y) \\log \\frac{p(x, y)}{p(x)}$\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Calculate the conditional entropy\n",
    "def cond_entro(var_list,class_var,data):\n",
    "    probs_class_var = dict(data[class_var].value_counts(normalize=True,sort=False))\n",
    "    var_list.append(class_var)\n",
    "    probs_join_var = dict(data.groupby(var_list).size())\n",
    "    join_sum = sum(probs_join_var.values())\n",
    "    probs_join_var = {key:value/join_sum for key,value in probs_join_var.items()}\n",
    "    entropy = 0 \n",
    "    for key,value in probs_join_var.items():\n",
    "        entropy -= value * np.log(value/probs_class_var[key[-1]])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mutual information between var1 and var2 given class_var by using conditional entropy\n",
    "def MI(var1,var2,class_var,data):\n",
    "    mutual_info = cond_entro([var1],class_var,data) + \\\n",
    "                  cond_entro([var2],class_var,data) - \\\n",
    "                  cond_entro([var1,var2],class_var,data)\n",
    "    return(mutual_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following two functions are for checking whehter a graph is cyclic\n",
    "def isCyclicUtil(v,visited,parent,MST):\n",
    "    visited[v] = True \n",
    "    for node in MST[v]:\n",
    "        if (visited[node]==False):\n",
    "            if(isCyclicUtil(node,visited,v,MST)):\n",
    "                return True\n",
    "        elif(parent!=node):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def isCyclic(MST,node_list):\n",
    "    visited = {node:False for node in node_list}\n",
    "    \n",
    "    for node in node_list:\n",
    "        if (visited[node]==False):\n",
    "            if(isCyclicUtil(node,visited,None,MST)==True):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Return a maximum spanning tree for given edges\n",
    "def max_spanning_tree(input_edges,outcomeSpace,class_var):\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    node_list.remove(\"Metastasis\")\n",
    "    node_list.remove(\"LymphNodes\")\n",
    "    MST = {node:[] for node in node_list}\n",
    "    node_list.remove(class_var)\n",
    "    edge_count = 0\n",
    "    edges = copy.deepcopy(input_edges)\n",
    "    edges.sort(reverse=True,key=lambda x:x[1])\n",
    "    \n",
    "    while(len(edges)>0):\n",
    "        edge = edges.pop(0)[0]\n",
    "        MST[edge[0]].append(edge[1])\n",
    "        MST[edge[1]].append(edge[0])\n",
    "        edge_count +=1\n",
    "        \n",
    "        if (isCyclic(MST,node_list)):\n",
    "            MST[edge[0]].remove(edge[1])\n",
    "            MST[edge[1]].remove(edge[0])\n",
    "            edge_count -=1\n",
    "        \n",
    "        if (edge_count == len(node_list)-1):\n",
    "            return (MST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Maximum Spanning Tree with directed edges\n",
    "def generate_directed_MST(G_tan,MST,node,node_count):\n",
    "    children = MST[node]\n",
    "    for child in children:\n",
    "        MST[child].remove(node)\n",
    "        G_tan,node_count = generate_directed_MST(G_tan,MST,child,node_count)\n",
    "        G_tan[node].append(child) \n",
    "    node_count+=1\n",
    "    return (G_tan,node_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a graph that is in TAN structure\n",
    "def learn_tan_structure(data,outcomeSpace,class_var):\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    node_list.remove(\"Metastasis\")\n",
    "    node_list.remove(\"LymphNodes\")\n",
    "    edges = []\n",
    "    G_tan = {node:[] for node in node_list}\n",
    "    node_list.remove(class_var)\n",
    "    for i in range(len(node_list)-1):\n",
    "        for j in range(i+1,len(node_list)):\n",
    "            mutual_info = MI(node_list[i],node_list[j],class_var,data)\n",
    "            edges.append(((node_list[i],node_list[j]),mutual_info))\n",
    "    MST = max_spanning_tree(edges,outcomeSpace,class_var)\n",
    "    node_count = 0\n",
    "    for node in node_list:\n",
    "        if (node_count == len(node_list)):\n",
    "            break\n",
    "        G_tan,node_count = generate_directed_MST(G_tan,MST,node,node_count)\n",
    "    G_tan[class_var] = node_list\n",
    "    return(G_tan)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time consumption is 1.34 seconds\n",
      "Passed test case\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "begin = time.time()\n",
    "tan_graph = learn_tan_structure(data, outcomeSpace, class_var)\n",
    "end = time.time()\n",
    "print(\"The time consumption is {0:.2f} seconds\".format(end-begin))\n",
    "test(len(tan_graph['BC']) == len(tan_graph)-1)\n",
    "test('FibrTissueDev' in tan_graph['Spiculation'] or 'Spiculation' in tan_graph['FibrTissueDev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the other tasks, design a function ``cv_tan(data, class_var)`` that uses 10-fold cross-validation to assess the performance of the TAN classifier from ``data``. Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy. This function should use the ``learn_tan_structure`` as well as other functions defined in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_tan(data, class_var) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a single line of data, predict the classification of the class_var\n",
    "def tan_prediction(tan_tables, single_data_dict,outcomeSpace,class_var):\n",
    "    prior = tan_tables[class_var]['table']\n",
    "    prob = {name : np.log(prior[tuple([name])]) for name in outcomeSpace[class_var]}\n",
    "    for key,value in tan_tables.items():\n",
    "        if (key==class_var):\n",
    "            continue\n",
    "        for name in outcomeSpace[class_var]:\n",
    "            evidence = []\n",
    "            for i in range(len(value['dom'])):\n",
    "                if (class_var == value['dom'][i]):\n",
    "                    evidence.append(name) \n",
    "                else:\n",
    "                    evidence.append(single_data_dict[value['dom'][i]])\n",
    "            evidence = tuple(evidence)\n",
    "            prob[name] += np.log(value['table'][evidence])\n",
    "    return (max(prob, key=prob.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the accuracy of Tree-augmented Bayes Classifier on the test data\n",
    "def assess_tan(G_tan,tan_tables,test_data,outcomeSpace,class_var):\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    var_remove = ['Metastasis', 'LymphNodes',class_var]\n",
    "    var_list = [i for i in node_list if i not in var_remove] #now we get all the variables \n",
    "    test_data_update = test_data[var_list]\n",
    "    test_data_dict = test_data_update.to_dict(orient='records')\n",
    "    outcomeSpace_copy = { var: outcomeSpace[var] for var in var_list}\n",
    "    match_count = 0\n",
    "    for i in range(len(test_data_dict)):\n",
    "        pred = tan_prediction(tan_tables,test_data_dict[i],outcomeSpace,class_var)\n",
    "        if (pred == test_data.iloc[i][class_var]):\n",
    "            match_count +=1\n",
    "    return (match_count/test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_tan(data,class_var):\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    fold_len = int(data.shape[0]/10)\n",
    "    acc_list = []\n",
    "    for i in range(10):\n",
    "        training_index = list(range(0,i*fold_len)) + list(range((i+1)*fold_len,data.shape[0]))\n",
    "        test_index = list(range(i*fold_len,(i+1)*fold_len))\n",
    "        training_data = data.iloc[training_index]\n",
    "        test_data = data.iloc[test_index]\n",
    "        G_tan = learn_tan_structure(training_data,outcomeSpace,class_var)\n",
    "        tan_tables = learn_bayes_net(G_tan, training_data, outcomeSpace)\n",
    "        acc_list.append(assess_tan(G_tan, tan_tables, test_data, outcomeSpace,class_var))\n",
    "    return (np.mean(acc_list),np.std(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time consumption is 19.76 seconds\n",
      "The accuracy is 0.8329, and the standard deviation is 0.01\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "acc, stdev = cv_tan(data,class_var)\n",
    "end = time.time()\n",
    "print(\"The time consumption is {0:.2f} seconds\".format(end-begin))\n",
    "print(\"The accuracy is {0:.4f}, and the standard deviation is {1:.2f}\".format(acc,stddev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 6 - Report\n",
    "\n",
    "Write a report (**with less than 500 words**) summarising your findings in this assignment. Your report should address the following:\n",
    "\n",
    "a. Make a summary and discussion of the experimental results (accuracy). Use plots to illustrate your results.\n",
    "\n",
    "b. Discuss the complexity of the implemented algorithms.\n",
    "\n",
    "Use Markdown and Latex to write your report in the Jupyter notebook. Develop some plots using Matplotlib to illustrate your results. Be mindful of the maximum number of words. Please, be concise and objective."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "**Running Environment**: MacBook Pro A1708\n",
    "\n",
    "## Results and discussion \n",
    "\n",
    "The accuracy and running time among the three methods are compared in the Figure below. Besides, these three methods have 0.01 standard deviation, which means all of them are stable.\n",
    "\n",
    "<img src=\"./imgs/bar_plot.png\" style=\"width: 400px;\"></img>\n",
    "\n",
    "According to the results, Bayesian network classifcation is outperforming the other two models with the highest accuracy. The reason is that with a specific causality graph, the code learns the associations between variables in depth and more precisely, so that irrelevant relations between variables will not be learnt and not affect the prediction. It is slower than Naive Bayes since joining tables is required. For the Naive Bayes classifier, it has the lowest accuracy since it has a strong assumption that variables are independent to each other, which is not always correct in reality. This strong assumption results in the classifier does not learn the correlations between attributes and fails to have a good comprehension on data. As for the Tree-augmented Classifier with $n$ nodes, $n-2$ attributes in it depend on the class and one another attribute, while one of them, as the head of a maximum spanning tree, only depends on the class. This classifier relaxes some independence restriction in Naive Bayes as attributes do not need to be irrelevant to each other, and plays a precise predictor role when the dependency structure is unknown as  proven by the results with an accuracy of 83.29%, which is almost 1% less than the Bayes Network. The reason why its running time is longer than the other two could be the classifier learns some useless information between the class variable and its irrelevant attributes, which consumes unnecessary time.\n",
    "\n",
    "Another important aspect of detection of breast cancer is about assigning different weights to false negative and false positive. Since there are three categories of BC, the accuracy in prediction given the true value in each categories are reported.\n",
    "\n",
    "**Bayes networks**\n",
    "<img src=\"./imgs/bayes.jpg\" style=\"width: 200px;\"></img>\n",
    "**Naive Bayes** \n",
    "<img src=\"./imgs/naive_bayes.jpg\" style=\"width: 200px;\"></img>\n",
    "**TAN**\n",
    "<img src=\"./imgs/tan.jpg\" style=\"width: 200px;\"></img>\n",
    "\n",
    "<br><br>\n",
    "False negative is a much more serious issue than False positive in medical detection as it leads to no treatments for patients.\n",
    "According to the table above, we can see some advantages of TAN compared to the other two models. \n",
    "For a patient with Insitu, TAN classifies 14% of them as  and for patient with Invasive, TAN calssifies 7 % of them as No, both of which are lower than the other two models\n",
    "Therefore, despite of slighly lower total accuracy of TAN, it is still preferred when we consider the serious outcomes of false negatives.\n",
    "\n",
    "### Complexity\n",
    "\n",
    "Assume that the size of data is $N$, the variable number is $G$, each variable has at most $m$ outcome spaces, the cross validation is $k$-fold.\n",
    "\n",
    "#### Bayesian Network\n",
    "\n",
    "- learn_outcome_space: $O(G)$ \n",
    "\n",
    "- learn_bayes_net: $O(G^3mN)$ \n",
    "- assess_bayes_net: $O(m^{G}N)$  \n",
    "\n",
    "In total ($k$-fold): $O(m^{G}Nk)$ \n",
    "\n",
    "#### Naive Bayes \n",
    "\n",
    "- learn_outcome_space: $O(G)$ \n",
    "- learn_naive_bayes: $O(G)$ \n",
    "- assess_naive_bayes: $O(NGm)$ \n",
    "\n",
    "In total ($k$-fold): $O(NGmk)$ \n",
    "\n",
    "#### Tree-augmented Bayes Classifier\n",
    "\n",
    "- learn_outcome_space: $O(G)$ \n",
    "- learn_tan_structure: $O(G^{2}N)$  \n",
    "- learn_bayes_net: $O(G^3mN)$ \n",
    "- assess_tan: $O(NGm)$ \n",
    "\n",
    "In total ($k$-fold): $O(mkNG)$ \n",
    "\n",
    "In Tree-augmented Bayes Classifier, if we use assess_bayes_net(), the running time could be 170 seconds because there are many tables to join due to the complexity of TAN graph structure. This is reflected by the complexity above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
