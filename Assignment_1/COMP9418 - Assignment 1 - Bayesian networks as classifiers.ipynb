{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9418 - Assignment 1 - Bayesian Networks as Classifiers\n",
    "\n",
    "## UNSW Sydney, October 2020\n",
    "\n",
    "- Student name 1 - zID\n",
    "- Student name 2 - ZID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "**Submission deadline:** Sunday, 18th October 2020, at 18:00:00.\n",
    "\n",
    "**Late Submission Policy:** The penalty is set at 20% per late day. This is ceiling penalty, so if a group is marked 60/100 and they submitted two days late, they still get 60/100.\n",
    "\n",
    "**Form of Submission:** This is a group assignment. Each group can have up to **two** students. **Only one member of the group should submit the assignment**.\n",
    "\n",
    "You can reuse any piece of source code developed in the tutorials.\n",
    "\n",
    "Submit your files using give. On a CSE Linux machine, type the following on the command-line:\n",
    "\n",
    "``$ give cs9418 ass1 solution.zip``\n",
    "\n",
    "Alternative, you can submit your solution via the [WebCMS](https://webcms3.cse.unsw.edu.au/COMP9418/20T3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical prerequisites\n",
    "\n",
    "These are the libraries your are allowed to use. No other libraries will be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make division default to floating-point, saving confusion\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Allowed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import heapq as pq\n",
    "import matplotlib as mp\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "from collections import OrderedDict as odict\n",
    "from graphviz import Digraph\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Supplemental libraries\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial task - Initialise graph\n",
    "\n",
    "Create a graph ``G`` that represents the following network by filling in the edge lists.\n",
    "![Bayes Net](BayesNet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = {\n",
    "    \"BreastDensity\" : [\"Mass\"],\n",
    "    \"Location\" : [\"BC\"],\n",
    "    \"Age\" : [\"BC\"],\n",
    "    \"BC\" : [\"Mass\", \"AD\", \"Metastasis\", \"MC\", \"SkinRetract\",\"NippleDischarge\"],\n",
    "    \"Mass\" : [\"Size\",  \"Shape\", \"Margin\" ],\n",
    "    \"AD\" : [\"FibrTissueDev\"],\n",
    "    \"Metastasis\" : [ \"LymphNodes\"],\n",
    "    \"MC\" : [],\n",
    "    \"Size\" : [],\n",
    "    \"Shape\" : [],\n",
    "    \"FibrTissueDev\" : [ \"SkinRetract\" , \"NippleDischarge\",\"Spiculation\" ],\n",
    "    \"LymphNodes\" : [],\n",
    "    \"SkinRetract\" : [],\n",
    "    \"NippleDischarge\" : [],\n",
    "    \"Spiculation\" : [\"Margin\" ],\n",
    "    \"Margin\" : [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 1 - Efficient d-separation test\n",
    "\n",
    "Implement the efficient version of the d-separation algorithm in a function ``d_separation(G, X, Z, Y)`` that return a boolean: true if **X** is d-separated from **Y** given **Z** in the graph $G$ and false otherwise.\n",
    "\n",
    "* **X**,**Y** and **Z** are python sets, each containing a set of variable names. \n",
    "* Variable names may be strings or integers, and can be assumed to be nodes of the graph $G$. \n",
    "* $G$ is a graph as defined in tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for d_separation(G, X, Z, Y) in one or more cells here\n",
    "import copy \n",
    "\n",
    "\n",
    "def isleaf_node(G,node):\n",
    "    return not G[node] \n",
    "\n",
    "def remove_leaf(G1,leaf_node):\n",
    "    # delete the leaf node and its edges from the G, return a new Graph\n",
    "    G_new = copy.deepcopy(G1)\n",
    "    del G_new[leaf_node]\n",
    "    for key, value in  G_new.items(): \n",
    "        if leaf_node in value:\n",
    "            G_new[key].remove(leaf_node)\n",
    "    return G_new\n",
    "    \n",
    "def repeat_del(G1,node_list): \n",
    "    count = 1 \n",
    "    list_update = node_list.copy()\n",
    "    while count > 0:\n",
    "        count = 0\n",
    "        for node in node_list: \n",
    "            if isleaf_node(G1,node):\n",
    "                #print(node)\n",
    "                #remove the nodes and update the graph \n",
    "                G1= copy.deepcopy(remove_leaf(G1,node))\n",
    "                #print(G)\n",
    "                list_update.remove(node)\n",
    "                count = count + 1 \n",
    "                #print(count)\n",
    "        node_list = list_update.copy()\n",
    "    return (G1)\n",
    "    \n",
    "\n",
    "    #if count == 0:\n",
    "    #    return(G)\n",
    "    #print(list_update)\n",
    "    #new_list = list_update.copy()\n",
    "    #G_new = repeat_del(G,new_list)\n",
    "    \n",
    " \n",
    "    \n",
    "def dfs_r(G, v, colour):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `G`, an adjacency list representation of a graph\n",
    "    `v`, next vertex to be visited\n",
    "    `colour`, dictionary with the colour of each node\n",
    "    \"\"\"\n",
    "    #print('Visiting: ', v)\n",
    "    # Visited vertices are coloured 'grey'\n",
    "    colour[v] = 'grey'\n",
    "    # Let's visit all outgoing edges from v\n",
    "    for w in G[v]:\n",
    "        # To avoid loops, we vist check if the next vertex hasn't been visited yet\n",
    "        if colour[w] == 'white':\n",
    "            dfs_r(G, w, colour)\n",
    "    # When we finish the for loop, we know we have visited all nodes from v. It is time to turn it 'black'\n",
    "    colour[v] = 'black' \n",
    "    return None\n",
    "  \n",
    "\n",
    "\n",
    "def d_separation(G1, X, Z, Y): \n",
    "    \"\"\" \n",
    "    Arguments: \n",
    "    `G`, an adjacency list representation of a graph \n",
    "    `X`, a set of variables name \n",
    "    `Y`, a set of variables name \n",
    "    `Z`, a set of a set of variables name \n",
    "    \n",
    "    Returns \n",
    "    a boolean: true if X is d-separated from Y given Z in the graph  ùê∫  and false otherwise.\n",
    "    \n",
    "    \"\"\"\n",
    "    if bool(X.intersection(Y).intersection(Z)):\n",
    "        print(\"X, Y, Z are not disjoint\")  #make it a warning/ error message? \n",
    "    \n",
    "    combine_set = X.union(Y).union(Z)\n",
    "    node_set = set(G1.keys())\n",
    "    remain_nodes = set(node_set  - combine_set)\n",
    "    \n",
    "    G_final = copy.deepcopy(repeat_del(G1,remain_nodes)) #repeat del leaf nodes \n",
    "    \n",
    "    for var in Z: \n",
    "        G_final[var] = [] #delete outgoing edges from Z set \n",
    "    \n",
    "    for key,values in G_final.items():\n",
    "        if bool(values):\n",
    "            for n in values: \n",
    "                G_final[n].append(key) #make it undirect graph\n",
    "\n",
    "    colour = {node: 'white' for node in G_final.keys()}\n",
    "    #check connectivity \n",
    "    separate = True \n",
    "    for nodex in X:\n",
    "        dfs_r(G_final,nodex,colour) \n",
    "        Y_color = [colour[node] for node in Y]\n",
    "        #print(Y_color)\n",
    "        if 'black' in Y_color:\n",
    "            separate = False\n",
    "            \n",
    "\n",
    "    return(separate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "def test(statement):\n",
    "    if statement:\n",
    "        print(\"Passed test case\")\n",
    "    else:\n",
    "        print(\"Failed test case\")\n",
    "        \n",
    "test(d_separation(G, set(['Age']), set(['BC']), set(['AD'])))\n",
    "test(not d_separation(G, set(['Spiculation','LymphNodes']), set(['MC', 'Size']), set(['Age'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Marks] Task 2 - Estimate Bayesian Network parameters from data\n",
    "\n",
    "Implement a function ``learn_outcome_space(data)`` that learns the outcome space (the valid values for each variable) from the pandas dataframe ``data`` and returns a dictionary ``outcomeSpace`` with these values.\n",
    "\n",
    "Implement a function ``learn_bayes_net(G, data, outcomeSpace)`` that learns the parameters of the Bayesian Network $G$. This function should return a dictionary ``prob_tables`` with the all conditional probability tables (one for each node).\n",
    "\n",
    "- ``G`` is a directed acyclic graph. For this part of the assignment, $G$ should be declared according to the breast cancer Bayesian network presented in the diagram in the assignment specification.\n",
    "- ``data`` is a dataframe created from a csv file containing the relevant data. \n",
    "- ``outcomeSpace`` is defined in tutorials.\n",
    "- ``prob_tables`` is a dict from each variable name (node) to a \"factor\". Factors are defined in tutorial 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_outcome_space(data) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_outcome_space(data):\n",
    "    outcomeSpace = {}\n",
    "    for attr in data.columns:\n",
    "        outcomeSpace[attr] = tuple(data[attr].unique())\n",
    "    return(outcomeSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "with open('bc.csv') as file:\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "outcomeSpace = learn_outcome_space(data)\n",
    "\n",
    "outcomes = outcomeSpace['BreastDensity']\n",
    "answer = ('high', 'medium', 'low')\n",
    "test(len(outcomes) == len(answer) and set(outcomes) == set(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_bayes_net(G, data, outcomeSpace) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxilliary functions\n",
    "def printFactor(f):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, a factor to print on screen\n",
    "    \"\"\"\n",
    "    # Create a empty list that we will fill in with the probability table entries\n",
    "    table = list()\n",
    "    \n",
    "    # Iterate over all keys and probability values in the table\n",
    "    for key, item in f['table'].items():\n",
    "        # Convert the tuple to a list to be able to manipulate it\n",
    "        k = list(key)\n",
    "        # Append the probability value to the list with key values\n",
    "        k.append(item)\n",
    "        # Append an entire row to the table\n",
    "        table.append(k)\n",
    "    # dom is used as table header. We need it converted to list\n",
    "    dom = list(f['dom'])\n",
    "    # Append a 'Pr' to indicate the probabity column\n",
    "    dom.append('Pr')\n",
    "    print(tabulate(table,headers=dom,tablefmt='fancy_grid'))\n",
    "    \n",
    "def prob(factor, *entry):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `factor`, a dictionary of domain and probability values,\n",
    "    `entry`, a list of values, one for each variable in the same order as specified in the factor domain.\n",
    "    \n",
    "    Returns p(entry)\n",
    "    \"\"\"\n",
    "\n",
    "    return factor['table'][entry]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allEqualThisIndex(dict_of_arrays,**fixed_vars):\n",
    "    first_array = dict_of_arrays[list(dict_of_arrays.keys())[0]]\n",
    "    index = np.ones_like(first_array,dtype=np.bool_)\n",
    "    for var_name,var_val in fixed_vars.items():\n",
    "        index = index & (np.asarray(dict_of_arrays[var_name])==var_val)\n",
    "    return (index)\n",
    "\n",
    "def estProbTable(data,var_name,parent_names,outcomeSpace,alpha=1):\n",
    "    var_outcomes = outcomeSpace[var_name]\n",
    "    parent_outcomes = [outcomeSpace[var] for var in parent_names]\n",
    "    all_parent_combinations = product(*parent_outcomes)\n",
    "    prob_table = odict()\n",
    "    \n",
    "    for i,parent_combination in enumerate(all_parent_combinations):\n",
    "        parent_vars = dict(zip(parent_names,parent_combination))\n",
    "        parent_index = allEqualThisIndex(data,**parent_vars)\n",
    "        for var_outcome in var_outcomes:\n",
    "            var_index = (np.asarray(data[var_name])==var_outcome)\n",
    "            new_dom = tuple(list(parent_combination)+[var_outcome])\n",
    "#             prob_table[new_dom]=(var_index & parent_index).sum()/parent_index.sum()\n",
    "            # Additive smoothing is applied here\n",
    "            \n",
    "            N = parent_index.sum()\n",
    "            c = (var_index & parent_index).sum()\n",
    "            X_cardinality = len(var_outcomes)\n",
    "            prob_table[new_dom] = (c+alpha)/(N+alpha*X_cardinality)\n",
    "    return({'dom':tuple(list(parent_names)+[var_name]),'table':prob_table})\n",
    "\n",
    "def transposeGraph(G):\n",
    "    GT = dict((v,[]) for v in G)\n",
    "    for v in G:\n",
    "        for w in G[v]:\n",
    "            GT[w].append(v)\n",
    "    return (GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_bayes_net(G,data,outcomeSpace):\n",
    "    bayes_net = odict()\n",
    "    GT = transposeGraph(G)\n",
    "    for child, parents in GT.items():\n",
    "        bayes_net[child] = estProbTable(data,child,parents,outcomeSpace)\n",
    "    return(bayes_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "prob_tables = learn_bayes_net(G, data, outcomeSpace)\n",
    "test(abs(prob_tables['Age']['table'][('35-49',)] - 0.2476) < 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 3 - Bayesian Network Classification\n",
    "\n",
    "Design a new function ``assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)`` that uses the test cases in ``data`` to assess the performance of the Bayesian network defined by ``G`` and ``prob_tables``. Implement the efficient classification procedure discussed in the lectures. Such a function should return the classifier accuracy. \n",
    " * ``class_var`` is the name of the variable you are predicting, using all other variables.\n",
    " * ``outcomeSpace`` was created in task 2\n",
    " \n",
    "Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy.\n",
    "\n",
    "Return just the accuracy:\n",
    "\n",
    "``acc = assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var) in one or more cells here\n",
    "def join(f1, f2, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f1`, first factor to be joined.\n",
    "    `f2`, second factor to be joined.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor with a join of f1 and f2\n",
    "    \"\"\"\n",
    "    \n",
    "    # First, we need to determine the domain of the new factor. It will be union of the domain in f1 and f2\n",
    "    # But it is important to eliminate the repetitions\n",
    "    common_vars = list(f1['dom']) + list(set(f2['dom']) - set(f1['dom']))\n",
    "    \n",
    "    # We will build a table from scratch, starting with an empty list. Later on, we will transform the list into a odict\n",
    "    table = list()\n",
    "    \n",
    "    # Here is where the magic happens. The product iterator will generate all combinations of varible values \n",
    "    # as specified in outcomeSpace. Therefore, it will naturally respect observed values\n",
    "    for entries in product(*[outcomeSpace[node] for node in common_vars]):\n",
    "        \n",
    "        # We need to map the entries to the domain of the factors f1 and f2\n",
    "        entryDict = dict(zip(common_vars, entries))\n",
    "        f1_entry = (entryDict[var] for var in f1['dom'])\n",
    "        f2_entry = (entryDict[var] for var in f2['dom'])\n",
    "        \n",
    "        # Insert your code here\n",
    "        p1 = prob(f1, *f1_entry)           # Use the fuction prob to calculate the probability in factor f1 for entry f1_entry \n",
    "        p2 = prob(f2, *f2_entry)           # Use the fuction prob to calculate the probability in factor f2 for entry f2_entry \n",
    "        \n",
    "        # Create a new table entry with the multiplication of p1 and p2\n",
    "        table.append((entries, p1 * p2))\n",
    "    return {'dom': tuple(common_vars), 'table': odict(table)}\n",
    "\n",
    "def p_joint(outcomeSpace, cond_tables):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `outcomeSpace`, dictionary with domain of each variable\n",
    "    `cond_tables`, conditional probability distributions estimated from data\n",
    "    \n",
    "    Returns a new factor with full joint distribution\n",
    "    \"\"\"    \n",
    "    \n",
    "    var_list = list(outcomeSpace.keys())\n",
    "    p = join(cond_tables[var_list[0]], cond_tables[var_list[1]], outcomeSpace)\n",
    "\n",
    "    for var in var_list[2:]:\n",
    "        p = join(p,cond_tables[var_list[var]],outcomeSpace)\n",
    "\n",
    "    return p\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_blanket(G,var):\n",
    "    \"\"\" determine the relevant varaibles given the var of interest, return a list of nodes \"\"\"\n",
    "    blanket_list = []\n",
    "    blanket_list = blanket_list + G[var] #include the children\n",
    "    children_list = blanket_list \n",
    "    GT = transposeGraph(G)\n",
    "    blanket_list = blanket_list + GT[var] #include the parents \n",
    "    \n",
    "    for node in children_list:\n",
    "        blanket_list = blanket_list + GT[node] #include spouse \n",
    "    \n",
    "    blanket_list = list(set(blanket_list))\n",
    "    blanket_list = [i for i in blanket_list if i != var]\n",
    "    return blanket_list\n",
    "    \n",
    "def p_joint_blanket(my_blanket, outcomeSpace, cond_tables):\n",
    "    var_list = my_blanket\n",
    "    \n",
    "    p = join(cond_tables[var_list[0]], cond_tables[var_list[1]], outcomeSpace)\n",
    "\n",
    "    for var in var_list[2:]:\n",
    "        p = join(p,cond_tables[var], outcomeSpace)\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidence(var, e, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `var`, a valid variable identifier.\n",
    "    `e`, the observed value for var.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns dictionary with a copy of outcomeSpace with var = e\n",
    "    \"\"\"    \n",
    "    newOutcomeSpace = outcomeSpace.copy()      # Make a copy of outcomeSpace with a copy to method copy(). 1 line\n",
    "    newOutcomeSpace[var] = (e,)                # Replace the domain of variable var with a tuple with a single element e. 1 line\n",
    "    return newOutcomeSpace\n",
    "\n",
    "def marginalize(f, var, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be marginalized.\n",
    "    `var`, variable to be summed out.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor f' with dom(f') = dom(f) - {var}\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Let's make a copy of f domain and convert it to a list. We need a list to be able to modify its elements\n",
    "    new_dom = list(f['dom'])\n",
    "    \n",
    "    new_dom.remove(var)            # Remove var from the list new_dom by calling the method remove(). 1 line\n",
    "    table = list()                 # Create an empty list for table. We will fill in table from scratch. 1 line\n",
    "    for entries in product(*[outcomeSpace[node] for node in new_dom]):\n",
    "        s = 0;                     # Initialize the summation variable s. 1 line\n",
    "\n",
    "        # We need to iterate over all possible outcomes of the variable var\n",
    "        for val in outcomeSpace[var]:\n",
    "            # To modify the tuple entries, we will need to convert it to a list\n",
    "            entriesList = list(entries)\n",
    "            # We need to insert the value of var in the right position in entriesList\n",
    "            entriesList.insert(f['dom'].index(var), val)\n",
    "                      \n",
    "            p = prob(f, *tuple(entriesList))     # Calculate the probability of factor f for entriesList. 1 line\n",
    "            s = s + p                            # Sum over all values of var by accumulating the sum in s. 1 line\n",
    "            \n",
    "        # Create a new table entry with the multiplication of p1 and p2\n",
    "        table.append((entries, s))\n",
    "    return {'dom': tuple(new_dom), 'table': odict(table)}\n",
    "\n",
    "\n",
    "def normalize(f):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be normalized.\n",
    "    \n",
    "    Returns a new factor f' as a copy of f with entries that sum up to 1\n",
    "    \"\"\" \n",
    "    table = list()\n",
    "    sum = 0\n",
    "    for k, p in f['table'].items():\n",
    "        sum = sum + p\n",
    "    for k, p in f['table'].items():\n",
    "        table.append((k, p/sum))\n",
    "    return {'dom': f['dom'], 'table': odict(table)}\n",
    "\n",
    "\n",
    "def query(p, outcomeSpace, q_vars, **q_evi):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `p`, probability table to query.\n",
    "    `outcomeSpace`, dictionary will variable domains\n",
    "    `q_vars`, list of variables in query head\n",
    "    `q_evi`, dictionary of evidence in the form of variables names and values\n",
    "    \n",
    "    Returns a new factor NORMALIZED factor will all hidden variables eliminated as evidence set as in q_evi\n",
    "    \"\"\"     \n",
    "    \n",
    "    # Let's make a copy of these structures, since we will reuse the variable names\n",
    "    pm = p.copy()\n",
    "    outSpace = outcomeSpace.copy()\n",
    "    \n",
    "    # First, we set the evidence \n",
    "    for var_evi, e in q_evi.items():\n",
    "        outSpace = evidence(var_evi, e, outSpace)\n",
    "        \n",
    "    # Second, we eliminate hidden variables NOT in the query\n",
    "    for var in outSpace:\n",
    "        if not var in q_vars:\n",
    "            pm = marginalize(pm, var, outSpace)\n",
    "    return normalize(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var):\n",
    "    \n",
    "    var_blanket = markov_blanket(G,class_var)\n",
    "    blanket_without_var = copy.deepcopy(var_blanket)\n",
    "    var_blanket.append(class_var)\n",
    "    var_remove = ['Metastasis', 'LymphNodes']\n",
    "    var_list = [i for i in var_blanket if i not in var_remove] # now we get the variables that needs for inference class_var \n",
    "    \n",
    "#     begin = time.time()\n",
    "    p_table =  p_joint_blanket(var_list, outcomeSpace, prob_tables)\n",
    "#     end = time.time()\n",
    "#     print(\"The joining table costs {0:.2f} seconds\".format(end-begin))\n",
    "    \n",
    "    \n",
    "    \n",
    "    q_var = class_var\n",
    "    evidence_list = [var for var in var_list if var!=class_var]\n",
    "    data_update = data[evidence_list]\n",
    "    \n",
    "    data_dict = data_update.to_dict(orient='records')\n",
    "    outcomeSpace_copy = { var: outcomeSpace[var] for var in var_list}\n",
    "    match_count = 0\n",
    "    for i in range(len(data_dict)):\n",
    "        q_table = query(p_table, outcomeSpace_copy, q_var, **data_dict[i])\n",
    "        pred = max(q_table['table'],key=q_table['table'].get)[0]\n",
    "        if (pred == data.iloc[i][q_var]):\n",
    "            match_count +=1\n",
    "    return (match_count/data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BreastDensity': 'high', 'Location': 'LolwOutQuad', 'Age': '35-49', 'BC': 'No', 'Mass': 'No', 'AD': 'No', 'Metastasis': 'no', 'MC': 'No', 'Size': '<1cm', 'Shape': 'Other', 'FibrTissueDev': 'No', 'LymphNodes': 'no', 'SkinRetract': 'No', 'NippleDischarge': 'No', 'Spiculation': 'No', 'Margin': 'Well-defined'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy = data.copy()\n",
    "data_dict = data_copy.to_dict(orient='records')\n",
    "data_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "class_var = \"BC\"\n",
    "acc = assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8423"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a function ``cv_bayes_net(G, data, class_var)`` that uses ``learn_outcome_space``, ``learn_bayes_net``and ``assess_bayes_net`` to learn and assess a Bayesian network in a dataset using 10-fold cross-validation. Compute and report the average accuracy over the ten cross-validation runs as well as the standard deviation, e.g.\n",
    "\n",
    "``acc, stddev = cv_bayes_net(G, data, class_var)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_bayes_net(G, data, class_var) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross validation is 10 fold here\n",
    "def cv_bayes_net(G,data,class_var):\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    fold_len = int(data.shape[0]/10)\n",
    "    acc_list = []\n",
    "    for i in range(10):\n",
    "        training_index = list(range(0,i*fold_len)) + list(range((i+1)*fold_len,data.shape[0]))\n",
    "        test_index = list(range(i*fold_len,(i+1)*fold_len))\n",
    "        training_data = data.iloc[training_index]\n",
    "        test_data = data.iloc[test_index]\n",
    "        prob_tables = learn_bayes_net(G,training_data,outcomeSpace)\n",
    "        acc_list.append(assess_bayes_net(G,prob_tables,test_data,outcomeSpace,class_var))\n",
    "    \n",
    "    print(acc_list)\n",
    "    return (np.mean(acc_list),np.std(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.841, 0.8445, 0.836, 0.848, 0.846, 0.843, 0.8285, 0.84, 0.837, 0.848]\n",
      "The time consumption is 7.37 seconds\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "import time\n",
    "begin = time.time()\n",
    "acc, stddev = cv_bayes_net(G, data, 'BC')\n",
    "end = time.time()\n",
    "print(\"The time consumption is {0:.2f} seconds\".format(end-begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8412000000000001"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Marks] Task 4 - Na√Øve Bayes Classification\n",
    "\n",
    "Design a new function ``assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var)`` to classify and assess the test cases in a dataset ``data`` according to the Na√Øve Bayes classifier. To classify each example, use the log probability trick discussed in the lectures. This function should return the accuracy of the classifier in ``data``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var) in one or more cells here\n",
    "\n",
    "def learn_naive_bayes_structure(outcomeSpace, class_var):\n",
    "    \"\"\"Return the naive-bayes graph structure (a dict) according to above info\"\"\"\n",
    "    G_nb = {}\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    node_list.remove(class_var)\n",
    "    \n",
    "    G_nb[class_var] = node_list\n",
    "    \n",
    "    for nodes in node_list:\n",
    "        G_nb[nodes] = []\n",
    "    \n",
    "    return G_nb\n",
    "\n",
    "def additive_smoothing(prop_tables,data,alpha=1):\n",
    "    N = data.shape[0]\n",
    "    tables = copy.deepcopy(prop_tables)\n",
    "    for node , table_dict in tables.items():\n",
    "        X_cardinality = len(table_dict['table'].values())\n",
    "        if (0 in table_dict['table'].values()): # addtive smoothing required\n",
    "            for nameSpace, prob in table_dict['table'].items():\n",
    "                count_X_eq_x = int(round(prob * N,0))\n",
    "                table_dict['table'][nameSpace] = (count_X_eq_x+alpha)/(N+alpha*X_cardinality)\n",
    "    return (tables)\n",
    "\n",
    "\n",
    "def single_var_query( e, node_table):\n",
    "    '''Return the log likelihood for each evidence variable '''\n",
    "    prob_with_evi = {key[0]: np.log(value) for key,value in node_table['table'].items() if key[1] == e} #np.log avoid error for log(0),, but it is slower\n",
    "    \n",
    "    return prob_with_evi\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def predict(x, y_space, table, prior):\n",
    "    # initialize the prediction dictionary by the prior probabilities\n",
    "    pre_dict = {i:  np.log(prior[i]) for i in y_space if prior[i]!=0}\n",
    "    for i in range(len(x)):\n",
    "        var_prob = single_var_query(x[i], table[x.index[i]])\n",
    "        for key in pre_dict.keys():\n",
    "            pre_dict[key] = pre_dict[key] +  var_prob[key] \n",
    "        \n",
    "    yhat = max(pre_dict, key=pre_dict.get)\n",
    "    \n",
    "    return yhat \n",
    "\n",
    "\n",
    "\n",
    "def assess_naive_bayes(G_naive, naive_tables, data, outcomeSpace, class_var):\n",
    "    naive_tables = additive_smoothing(naive_tables,data)\n",
    "    \n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    var_remove = ['Metastasis', 'LymphNodes']\n",
    "    var_list = [i for i in node_list if i not in var_remove] #now we get all the variables \n",
    "    \n",
    "    evidence_list = [var for var in var_list if var!=class_var]\n",
    "    data_update = data[evidence_list]\n",
    "    \n",
    "    prior_prob = data[class_var].value_counts(normalize = True)   \n",
    "    y_hat_series = data_update.apply(predict, y_space = outcomeSpace[class_var], table = naive_tables, prior = prior_prob, axis = 1)\n",
    "    correct_predict = np.sum(data[class_var] == y_hat_series)\n",
    "    acc = correct_predict/len( y_hat_series)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7926"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "G_naive = learn_naive_bayes_structure(outcomeSpace,class_var)\n",
    "naive_tables = learn_bayes_net(G_naive, data, outcomeSpace)\n",
    "acc = assess_naive_bayes(G_naive, naive_tables, data, outcomeSpace, 'BC')\n",
    "acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a new function ``cv_naive_bayes(data, class_var)`` that uses ``assess_naive_bayes`` to assess the performance of the Na√Øve Bayes classifier in a dataset ``data``. To develop this code, perform the following steps:\n",
    "\n",
    "1. Use 10-fold cross-validation to split the data into training and test sets.\n",
    "\n",
    "2. Implement a function ``learn_naive_bayes_structure(outcomeSpace, class_var)`` to create and return a Na√Øve Bayes graph structure from ``outcomeSpace`` and ``class_var``. \n",
    "\n",
    "3. Use ``learn_bayes_net(G, data, outcomeSpace)`` to learn the Na√Øve Bayes parameters from a training set ``data``. \n",
    "\n",
    "4. Use ``assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var)`` to compute the accuracy of the Na√Øve Bayes classifier in a test set ``data``. Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy.\n",
    "\n",
    "Do 10-fold cross-validation, same as above, and return ``acc`` and ``stddev``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_naive_bayes(data, class_var) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_naive_bayes(data,class_var):\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    fold_len = int(data.shape[0]/10)\n",
    "    acc_list = []\n",
    "    G_naive = learn_naive_bayes_structure(outcomeSpace,class_var)\n",
    "    for i in range(10):\n",
    "        training_index = list(range(0,i*fold_len)) + list(range((i+1)*fold_len,data.shape[0]))\n",
    "        test_index = list(range(i*fold_len,(i+1)*fold_len))\n",
    "        training_data = data.iloc[training_index]\n",
    "        test_data = data.iloc[test_index]\n",
    "        naive_tables = learn_bayes_net(G_naive, training_data, outcomeSpace)\n",
    "        acc_list.append(assess_naive_bayes(G_naive, naive_tables, test_data, outcomeSpace,class_var))\n",
    "    return (np.mean(acc_list),np.std(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time consumption is 5.4661760330200195 seconds\n",
      "0.7919499999999999 0.00580603134679791\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "begin = time.time()\n",
    "acc, stdev = cv_naive_bayes(data,class_var)\n",
    "end = time.time()\n",
    "print(\"The time consumption is {0} seconds\".format(end-begin))\n",
    "print(acc,stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 5 - Tree-augmented Na√Øve Bayes Classification\n",
    "\n",
    "Similarly to the previous task, implement a Tree-augmented Na√Øve Bayes (TAN) classifier and evaluate your implementation in the breast cancer dataset. Design a function ``learn_tan_structure(data, outcomeSpace, class_var)`` to learn the TAN structure (graph) from the ``data`` and returns such a structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note by Yangqi : The following functions are used to enable more efficient (maybe) computation for mutual information \n",
    "\n",
    "To calculate conditional mutual info: \n",
    "https://en.wikipedia.org/wiki/Conditional_mutual_information#Some_identities\n",
    "\n",
    "\n",
    "$I(X;Y|Z) = H(X|Z) + H(Y|Z) - H(X,Y|Z)$\n",
    "\n",
    "$\\mathrm{H}(Y \\mid X)=-\\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} p(x, y) \\log \\frac{p(x, y)}{p(x)}$\n",
    "\n",
    "using the Entropy identity: we can calculate it easily. To calculate entropy, we need to know the distribution for each jotint var, \n",
    "df.value_counts will be usful \n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.value_counts.html\n",
    "\n",
    "Then we simply calculate entropy and workout the conditional mutual info. \n",
    "\n",
    "https://stackoverflow.com/questions/49685591/how-to-find-the-entropy-of-each-column-of-data-set-by-python\n",
    "\n",
    "After that, Bayesian graph can be quickly obtain by max span tree \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_entro(var_list,class_var,data):\n",
    "    probs_class_var = dict(data[class_var].value_counts(normalize=True,sort=False))\n",
    "    var_list.append(class_var)\n",
    "    probs_join_var = dict(data.groupby(var_list).size())\n",
    "    join_sum = sum(probs_join_var.values())\n",
    "    probs_join_var = {key:value/join_sum for key,value in probs_join_var.items()}\n",
    "    entropy = 0 \n",
    "    for key,value in probs_join_var.items():\n",
    "        entropy -= value * np.log(value/probs_class_var[key[-1]])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MI(var1,var2,class_var,data):\n",
    "    mutual_info = cond_entro([var1],class_var,data) + \\\n",
    "                  cond_entro([var2],class_var,data) - \\\n",
    "                  cond_entro([var1,var2],class_var,data)\n",
    "    return(mutual_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isCyclicUtil(v,visited,parent,MST):\n",
    "    visited[v] = True\n",
    "    \n",
    "    for node in MST[v]:\n",
    "        if (visited[node]==False):\n",
    "            if(isCyclicUtil(node,visited,v,MST)):\n",
    "                return True\n",
    "        elif(parent!=node):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def isCyclic(MST,node_list):\n",
    "    visited = {node:False for node in node_list}\n",
    "    \n",
    "    for node in node_list:\n",
    "        if (visited[node]==False):\n",
    "            if(isCyclicUtil(node,visited,None,MST)==True):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def max_spanning_tree(input_edges,outcomeSpace,class_var):\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    node_list.remove(\"Metastasis\")\n",
    "    node_list.remove(\"LymphNodes\")\n",
    "    MST = {node:[] for node in node_list}\n",
    "    node_list.remove(class_var)\n",
    "    edge_count = 0\n",
    "    edges = copy.deepcopy(input_edges)\n",
    "    edges.sort(reverse=True,key=lambda x:x[1])\n",
    "    \n",
    "    while(len(edges)>0):\n",
    "        edge = edges.pop(0)[0]\n",
    "        MST[edge[0]].append(edge[1])\n",
    "        MST[edge[1]].append(edge[0])\n",
    "        edge_count +=1\n",
    "        \n",
    "        if (isCyclic(MST,node_list)):\n",
    "            MST[edge[0]].remove(edge[1])\n",
    "            MST[edge[1]].remove(edge[0])\n",
    "            edge_count -=1\n",
    "        \n",
    "        if (edge_count == len(node_list)-1):\n",
    "            return (MST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_directed_MST(G_tan,MST,node,node_count):\n",
    "    children = MST[node]\n",
    "    for child in children:\n",
    "        MST[child].remove(node)\n",
    "        G_tan,node_count = generate_directed_MST(G_tan,MST,child,node_count)\n",
    "        G_tan[node].append(child) \n",
    "    node_count+=1\n",
    "    return (G_tan,node_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_tan_structure(data,outcomeSpace,class_var):\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    node_list.remove(\"Metastasis\")\n",
    "    node_list.remove(\"LymphNodes\")\n",
    "    edges = []\n",
    "    G_tan = {node:[] for node in node_list}\n",
    "    node_list.remove(class_var)\n",
    "    for i in range(len(node_list)-1):\n",
    "        for j in range(i+1,len(node_list)):\n",
    "            mutual_info = MI(node_list[i],node_list[j],class_var,data)\n",
    "            edges.append(((node_list[i],node_list[j]),mutual_info))\n",
    "    MST = max_spanning_tree(edges,outcomeSpace,class_var)\n",
    "    node_count = 0\n",
    "    for node in node_list:\n",
    "        if (node_count == len(node_list)):\n",
    "            break\n",
    "        G_tan,node_count = generate_directed_MST(G_tan,MST,node,node_count)\n",
    "    G_tan[class_var] = node_list\n",
    "    return(G_tan)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time consumption is 1.83 seconds\n",
      "Passed test case\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "############\n",
    "## TEST CODE\n",
    "begin = time.time()\n",
    "tan_graph = learn_tan_structure(data, outcomeSpace, class_var)\n",
    "end = time.time()\n",
    "print(\"The time consumption is {0:.2f} seconds\".format(end-begin))\n",
    "test(len(tan_graph['BC']) == len(tan_graph)-1)\n",
    "test('FibrTissueDev' in tan_graph['Spiculation'] or 'Spiculation' in tan_graph['FibrTissueDev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BreastDensity': ['Mass', 'Location'], 'Location': ['Age'], 'Age': [], 'BC': ['BreastDensity', 'Location', 'Age', 'Mass', 'AD', 'MC', 'Size', 'Shape', 'FibrTissueDev', 'SkinRetract', 'NippleDischarge', 'Spiculation', 'Margin'], 'Mass': ['Shape', 'Size', 'Margin'], 'AD': [], 'MC': [], 'Size': [], 'Shape': ['MC'], 'FibrTissueDev': ['SkinRetract', 'NippleDischarge', 'AD'], 'SkinRetract': [], 'NippleDischarge': [], 'Spiculation': ['FibrTissueDev'], 'Margin': ['Spiculation']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tan_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tan_tables = learn_bayes_net(tan_graph,data, outcomeSpace)\n",
    "tan_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ={1:2,3:4,5:6}\n",
    "3 in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tan_tables(tan_tables):\n",
    "    new_table = odict()\n",
    "    for key, value in tan_tables.items(): #key is the var name and value is its dictionary\n",
    "        new_sub_table = odict()\n",
    "        for sub_key, sub_value in value['table'].items():\n",
    "            new_sub_key = tuple(list(sub_key)[:-1])\n",
    "            q_var = sub_key[-1]\n",
    "            if (new_sub_key not in new_sub_table):\n",
    "                new_sub_table[new_sub_key] = odict()\n",
    "            new_sub_table[new_sub_key][q_var] = sub_value\n",
    "        new_table[key] = {'dom':value['dom'],'table':new_sub_table}\n",
    "    return (new_table)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('No', 0.6219067139929011), ('Invasive', 0.23616457531370294), ('Insitu', 0.141928710693396)])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tan_tables = update_tan_tables(tan_tables)\n",
    "new_tan_tables['BC']['table'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1]\n",
    "tuple(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('BreastDensity', {'dom': ('BC', 'BreastDensity'), 'table': OrderedDict([(('No',), OrderedDict([('high', 0.30212184536248193), ('medium', 0.4967047098537213), ('low', 0.2011734447837968)])), (('Invasive',), OrderedDict([('high', 0.3000423190859077), ('medium', 0.5021159542953872), ('low', 0.19784172661870503)])), (('Insitu',), OrderedDict([('high', 0.30095036958817317), ('medium', 0.504047870468145), ('low', 0.1950017599436818)]))])}), ('Location', {'dom': ('BreastDensity', 'BC', 'Location'), 'table': OrderedDict([(('high', 'No'), OrderedDict([('LolwOutQuad', 0.27246145667198296), ('UpOutQuad', 0.2307283359914939), ('UpInQuad', 0.2395002658160553), ('LowInQuad', 0.2573099415204678)])), (('high', 'Invasive'), OrderedDict([('LolwOutQuad', 0.2315270935960591), ('UpOutQuad', 0.235045742434905), ('UpInQuad', 0.26460239268121044), ('LowInQuad', 0.26882477128782545)])), (('high', 'Insitu'), OrderedDict([('LolwOutQuad', 0.18997668997668998), ('UpOutQuad', 0.3041958041958042), ('UpInQuad', 0.28088578088578087), ('LowInQuad', 0.22494172494172493)])), (('medium', 'No'), OrderedDict([('LolwOutQuad', 0.27252142972666993), ('UpOutQuad', 0.22513343037360506), ('UpInQuad', 0.24276241306808993), ('LowInQuad', 0.25958272683163514)])), (('medium', 'Invasive'), OrderedDict([('LolwOutQuad', 0.22053872053872053), ('UpOutQuad', 0.26304713804713803), ('UpInQuad', 0.2537878787878788), ('LowInQuad', 0.26262626262626265)])), (('medium', 'Insitu'), OrderedDict([('LolwOutQuad', 0.22229965156794426), ('UpOutQuad', 0.32125435540069686), ('UpInQuad', 0.2745644599303136), ('LowInQuad', 0.1818815331010453)])), (('low', 'No'), OrderedDict([('LolwOutQuad', 0.24501197126895452), ('UpOutQuad', 0.22905027932960895), ('UpInQuad', 0.25977653631284914), ('LowInQuad', 0.26616121308858737)])), (('low', 'Invasive'), OrderedDict([('LolwOutQuad', 0.2505330490405117), ('UpOutQuad', 0.24733475479744135), ('UpInQuad', 0.27398720682302774), ('LowInQuad', 0.2281449893390192)])), (('low', 'Insitu'), OrderedDict([('LolwOutQuad', 0.2459605026929982), ('UpOutQuad', 0.3339317773788151), ('UpInQuad', 0.22262118491921004), ('LowInQuad', 0.19748653500897667)]))])}), ('Age', {'dom': ('Location', 'BC', 'Age'), 'table': OrderedDict([(('LolwOutQuad', 'No'), OrderedDict([('35-49', 0.2796992481203007), ('50-74', 0.41654135338345866), ('>75', 0.1449624060150376), ('<35', 0.15879699248120302)])), (('LolwOutQuad', 'Invasive'), OrderedDict([('35-49', 0.15794306703397612), ('50-74', 0.6978879706152433), ('>75', 0.13957759412304868), ('<35', 0.004591368227731864)])), (('LolwOutQuad', 'Insitu'), OrderedDict([('35-49', 0.19193548387096773), ('50-74', 0.6096774193548387), ('>75', 0.19193548387096773), ('<35', 0.0064516129032258064)])), (('UpOutQuad', 'No'), OrderedDict([('35-49', 0.23985890652557318), ('50-74', 0.4345679012345679), ('>75', 0.15661375661375662), ('<35', 0.1689594356261023)])), (('UpOutQuad', 'Invasive'), OrderedDict([('35-49', 0.2080536912751678), ('50-74', 0.62751677852349), ('>75', 0.1535234899328859), ('<35', 0.010906040268456376)])), (('UpOutQuad', 'Insitu'), OrderedDict([('35-49', 0.3487348734873487), ('50-74', 0.5346534653465347), ('>75', 0.09900990099009901), ('<35', 0.0176017601760176)])), (('UpInQuad', 'No'), OrderedDict([('35-49', 0.27079240340537003), ('50-74', 0.4034053700065488), ('>75', 0.16208251473477406), ('<35', 0.16371971185330714)])), (('UpInQuad', 'Invasive'), OrderedDict([('35-49', 0.16491511721907842), ('50-74', 0.6968472109943411), ('>75', 0.12368633791430882), ('<35', 0.014551333872271624)])), (('UpInQuad', 'Insitu'), OrderedDict([('35-49', 0.3105263157894737), ('50-74', 0.5302631578947369), ('>75', 0.14342105263157895), ('<35', 0.015789473684210527)])), (('LowInQuad', 'No'), OrderedDict([('35-49', 0.26164763961740206), ('50-74', 0.42178340018512805), ('>75', 0.16322122801604444), ('<35', 0.15334773218142547)])), (('LowInQuad', 'Invasive'), OrderedDict([('35-49', 0.14414414414414414), ('50-74', 0.7354627354627354), ('>75', 0.11302211302211303), ('<35', 0.007371007371007371)])), (('LowInQuad', 'Insitu'), OrderedDict([('35-49', 0.3805309734513274), ('50-74', 0.47079646017699117), ('>75', 0.13097345132743363), ('<35', 0.017699115044247787)]))])}), ('BC', {'dom': ('BC',), 'table': OrderedDict([((), OrderedDict([('No', 0.6219067139929011), ('Invasive', 0.23616457531370294), ('Insitu', 0.141928710693396)]))])}), ('Mass', {'dom': ('BreastDensity', 'BC', 'Mass'), 'table': OrderedDict([(('high', 'No'), OrderedDict([('No', 0.8495081095453337), ('Benign', 0.15022600372241426), ('Malign', 0.00026588673225206064)])), (('high', 'Invasive'), OrderedDict([('No', 0.1056338028169014), ('Benign', 0.09718309859154929), ('Malign', 0.7971830985915493)])), (('high', 'Insitu'), OrderedDict([('No', 0.20070011668611434), ('Benign', 0.4072345390898483), ('Malign', 0.3920653442240373)])), (('medium', 'No'), OrderedDict([('No', 0.8919443545778065), ('Benign', 0.10789388547395665), ('Malign', 0.00016175994823681658)])), (('medium', 'Invasive'), OrderedDict([('No', 0.20589473684210527), ('Benign', 0.1637894736842105), ('Malign', 0.6303157894736842)])), (('medium', 'Insitu'), OrderedDict([('No', 0.2601115760111576), ('Benign', 0.39748953974895396), ('Malign', 0.3423988842398884)])), (('low', 'No'), OrderedDict([('No', 0.9413173652694611), ('Benign', 0.058283433133732535), ('Malign', 0.0003992015968063872)])), (('low', 'Invasive'), OrderedDict([('No', 0.26680896478121663), ('Benign', 0.18783351120597652), ('Malign', 0.5453575240128068)])), (('low', 'Insitu'), OrderedDict([('No', 0.24280575539568344), ('Benign', 0.44064748201438847), ('Malign', 0.31654676258992803)]))])}), ('AD', {'dom': ('BC', 'FibrTissueDev', 'AD'), 'table': OrderedDict([(('No', 'No'), OrderedDict([('No', 0.9796696315120712), ('Yes', 0.020330368487928845)])), (('No', 'Yes'), OrderedDict([('No', 0.8932866827028209), ('Yes', 0.10671331729717909)])), (('Invasive', 'No'), OrderedDict([('No', 0.7433789954337899), ('Yes', 0.25662100456621006)])), (('Invasive', 'Yes'), OrderedDict([('No', 0.37761135199054), ('Yes', 0.62238864800946)])), (('Insitu', 'No'), OrderedDict([('No', 0.8628608923884514), ('Yes', 0.13713910761154854)])), (('Insitu', 'Yes'), OrderedDict([('No', 0.5189681335356601), ('Yes', 0.48103186646433993)]))])}), ('MC', {'dom': ('BC', 'Shape', 'MC'), 'table': OrderedDict([(('No', 'Other'), OrderedDict([('No', 0.9718107550049376), ('Yes', 0.028189244995062392)])), (('No', 'Oval'), OrderedDict([('No', 0.9815950920245399), ('Yes', 0.018404907975460124)])), (('No', 'Round'), OrderedDict([('No', 0.9688542825361512), ('Yes', 0.03114571746384872)])), (('No', 'Irregular'), OrderedDict([('No', 0.9518072289156626), ('Yes', 0.04819277108433735)])), (('Invasive', 'Other'), OrderedDict([('No', 0.5281995661605207), ('Yes', 0.4718004338394794)])), (('Invasive', 'Oval'), OrderedDict([('No', 0.5390505359877489), ('Yes', 0.4609494640122512)])), (('Invasive', 'Round'), OrderedDict([('No', 0.5076335877862596), ('Yes', 0.49236641221374045)])), (('Invasive', 'Irregular'), OrderedDict([('No', 0.5371308016877637), ('Yes', 0.4628691983122363)])), (('Insitu', 'Other'), OrderedDict([('No', 0.4980026631158455), ('Yes', 0.5019973368841545)])), (('Insitu', 'Oval'), OrderedDict([('No', 0.4610091743119266), ('Yes', 0.5389908256880734)])), (('Insitu', 'Round'), OrderedDict([('No', 0.4619815668202765), ('Yes', 0.5380184331797235)])), (('Insitu', 'Irregular'), OrderedDict([('No', 0.5195954487989887), ('Yes', 0.4804045512010114)]))])}), ('Size', {'dom': ('BC', 'Mass', 'Size'), 'table': OrderedDict([(('No', 'No'), OrderedDict([('<1cm', 0.9998192825517304), ('1-3cm', 9.035872413481521e-05), ('>3cm', 9.035872413481521e-05)])), (('No', 'Benign'), OrderedDict([('<1cm', 0.11828737300435414), ('1-3cm', 0.2525399129172714), ('>3cm', 0.6291727140783745)])), (('No', 'Malign'), OrderedDict([('<1cm', 0.3333333333333333), ('1-3cm', 0.3333333333333333), ('>3cm', 0.3333333333333333)])), (('Invasive', 'No'), OrderedDict([('<1cm', 0.9977502812148481), ('1-3cm', 0.0011248593925759281), ('>3cm', 0.0011248593925759281)])), (('Invasive', 'Benign'), OrderedDict([('<1cm', 0.09246088193456614), ('1-3cm', 0.24039829302987198), ('>3cm', 0.6671408250355618)])), (('Invasive', 'Malign'), OrderedDict([('<1cm', 0.28821656050955413), ('1-3cm', 0.5621019108280255), ('>3cm', 0.14968152866242038)])), (('Insitu', 'No'), OrderedDict([('<1cm', 0.9970588235294118), ('1-3cm', 0.0014705882352941176), ('>3cm', 0.0014705882352941176)])), (('Insitu', 'Benign'), OrderedDict([('<1cm', 0.09450171821305842), ('1-3cm', 0.27233676975945015), ('>3cm', 0.6331615120274914)])), (('Insitu', 'Malign'), OrderedDict([('<1cm', 0.28414755732801594), ('1-3cm', 0.5543369890329013), ('>3cm', 0.16151545363908276)]))])}), ('Shape', {'dom': ('BC', 'Mass', 'Shape'), 'table': OrderedDict([(('No', 'No'), OrderedDict([('Other', 0.9997289483194796), ('Oval', 9.035056017347308e-05), ('Round', 9.035056017347308e-05), ('Irregular', 9.035056017347308e-05)])), (('No', 'Benign'), OrderedDict([('Other', 0.05366207396664249), ('Oval', 0.23567802755620015), ('Round', 0.6511965192168238), ('Irregular', 0.05946337926033358)])), (('No', 'Malign'), OrderedDict([('Other', 0.25), ('Oval', 0.25), ('Round', 0.25), ('Irregular', 0.25)])), (('Invasive', 'No'), OrderedDict([('Other', 0.996629213483146), ('Oval', 0.0011235955056179776), ('Round', 0.0011235955056179776), ('Irregular', 0.0011235955056179776)])), (('Invasive', 'Benign'), OrderedDict([('Other', 0.04971590909090909), ('Oval', 0.24431818181818182), ('Round', 0.6463068181818182), ('Irregular', 0.05965909090909091)])), (('Invasive', 'Malign'), OrderedDict([('Other', 0.0003183699458771092), ('Oval', 0.15313594396688954), ('Round', 0.10538045208532315), ('Irregular', 0.7411652340019103)])), (('Insitu', 'No'), OrderedDict([('Other', 0.9955947136563876), ('Oval', 0.0014684287812041115), ('Round', 0.0014684287812041115), ('Irregular', 0.0014684287812041115)])), (('Insitu', 'Benign'), OrderedDict([('Other', 0.06266094420600858), ('Oval', 0.24034334763948498), ('Round', 0.6549356223175966), ('Irregular', 0.04206008583690987)])), (('Insitu', 'Malign'), OrderedDict([('Other', 0.00099601593625498), ('Oval', 0.1553784860557769), ('Round', 0.10458167330677291), ('Irregular', 0.7390438247011952)]))])}), ('FibrTissueDev', {'dom': ('BC', 'Spiculation', 'FibrTissueDev'), 'table': OrderedDict([(('No', 'No'), OrderedDict([('No', 0.8535398796568942), ('Yes', 0.14646012034310588)])), (('No', 'Yes'), OrderedDict([('No', 0.25971502590673573), ('Yes', 0.7402849740932642)])), (('Invasive', 'No'), OrderedDict([('No', 0.7368421052631579), ('Yes', 0.2631578947368421)])), (('Invasive', 'Yes'), OrderedDict([('No', 0.1439706556625401), ('Yes', 0.8560293443374599)])), (('Insitu', 'No'), OrderedDict([('No', 0.7946374162096282), ('Yes', 0.20536258379037173)])), (('Insitu', 'Yes'), OrderedDict([('No', 0.18318068276436303), ('Yes', 0.8168193172356369)]))])}), ('SkinRetract', {'dom': ('BC', 'FibrTissueDev', 'SkinRetract'), 'table': OrderedDict([(('No', 'No'), OrderedDict([('No', 0.9512071156289708), ('Yes', 0.04879288437102922)])), (('No', 'Yes'), OrderedDict([('No', 0.653619068445222), ('Yes', 0.34638093155477806)])), (('Invasive', 'No'), OrderedDict([('No', 0.6438356164383562), ('Yes', 0.3561643835616438)])), (('Invasive', 'Yes'), OrderedDict([('No', 0.15687820260149785), ('Yes', 0.8431217973985021)])), (('Insitu', 'No'), OrderedDict([('No', 0.7559055118110236), ('Yes', 0.2440944881889764)])), (('Insitu', 'Yes'), OrderedDict([('No', 0.3277693474962064), ('Yes', 0.6722306525037937)]))])}), ('NippleDischarge', {'dom': ('BC', 'FibrTissueDev', 'NippleDischarge'), 'table': OrderedDict([(('No', 'No'), OrderedDict([('No', 0.9506988564167725), ('Yes', 0.049301143583227444)])), (('No', 'Yes'), OrderedDict([('No', 0.6643341351410452), ('Yes', 0.3356658648589547)])), (('Invasive', 'No'), OrderedDict([('No', 0.6639269406392694), ('Yes', 0.3360730593607306)])), (('Invasive', 'Yes'), OrderedDict([('No', 0.15372487189594009), ('Yes', 0.8462751281040599)])), (('Insitu', 'No'), OrderedDict([('No', 0.7683727034120735), ('Yes', 0.2316272965879265)])), (('Insitu', 'Yes'), OrderedDict([('No', 0.36646433990895294), ('Yes', 0.633535660091047)]))])}), ('Spiculation', {'dom': ('BC', 'Margin', 'Spiculation'), 'table': OrderedDict([(('No', 'Well-defined'), OrderedDict([('No', 0.9774455471065859), ('Yes', 0.0225544528934141)])), (('No', 'Ill-defined'), OrderedDict([('No', 0.048462852263023055), ('Yes', 0.951537147736977)])), (('Invasive', 'Well-defined'), OrderedDict([('No', 0.8408408408408409), ('Yes', 0.15915915915915915)])), (('Invasive', 'Ill-defined'), OrderedDict([('No', 0.4200294550810015), ('Yes', 0.5799705449189986)])), (('Insitu', 'Well-defined'), OrderedDict([('No', 0.8379478827361564), ('Yes', 0.16205211726384364)])), (('Insitu', 'Ill-defined'), OrderedDict([('No', 0.379182156133829), ('Yes', 0.620817843866171)]))])}), ('Margin', {'dom': ('BC', 'Mass', 'Margin'), 'table': OrderedDict([(('No', 'No'), OrderedDict([('Well-defined', 0.6290439183083318), ('Ill-defined', 0.37095608169166816)])), (('No', 'Benign'), OrderedDict([('Well-defined', 0.579520697167756), ('Ill-defined', 0.420479302832244)])), (('No', 'Malign'), OrderedDict([('Well-defined', 0.5), ('Ill-defined', 0.5)])), (('Invasive', 'No'), OrderedDict([('Well-defined', 0.5371621621621622), ('Ill-defined', 0.46283783783783783)])), (('Invasive', 'Benign'), OrderedDict([('Well-defined', 0.5968660968660968), ('Ill-defined', 0.4031339031339031)])), (('Invasive', 'Malign'), OrderedDict([('Well-defined', 0.13921631092704684), ('Ill-defined', 0.8607836890729532)])), (('Insitu', 'No'), OrderedDict([('Well-defined', 0.6067746686303387), ('Ill-defined', 0.39322533136966126)])), (('Insitu', 'Benign'), OrderedDict([('Well-defined', 0.587274290627687), ('Ill-defined', 0.412725709372313)])), (('Insitu', 'Malign'), OrderedDict([('Well-defined', 0.13373253493013973), ('Ill-defined', 0.8662674650698603)]))])})])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tan_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BreastDensity ('BC', 'BreastDensity')\n",
      "Location ('BreastDensity', 'BC', 'Location')\n",
      "Age ('Location', 'BC', 'Age')\n",
      "BC ('BC',)\n",
      "Mass ('BreastDensity', 'BC', 'Mass')\n",
      "AD ('BC', 'FibrTissueDev', 'AD')\n",
      "MC ('BC', 'Shape', 'MC')\n",
      "Size ('BC', 'Mass', 'Size')\n",
      "Shape ('BC', 'Mass', 'Shape')\n",
      "FibrTissueDev ('BC', 'Spiculation', 'FibrTissueDev')\n",
      "SkinRetract ('BC', 'FibrTissueDev', 'SkinRetract')\n",
      "NippleDischarge ('BC', 'FibrTissueDev', 'NippleDischarge')\n",
      "Spiculation ('BC', 'Margin', 'Spiculation')\n",
      "Margin ('BC', 'Mass', 'Margin')\n"
     ]
    }
   ],
   "source": [
    "for key, value in new_tan_tables.items():\n",
    "    print(key,value['dom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('No', 'Invasive', 'Insitu')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomeSpace['BC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the other tasks, design a function ``cv_tan(data, class_var)`` that uses 10-fold cross-validation to assess the performance of the TAN classifier from ``data``. Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy. This function should use the ``learn_tan_structure`` as well as other functions defined in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'No': 0.62195, 'Invasive': 0.23615, 'Insitu': 0.1419}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_prob = dict(data[class_var].value_counts(normalize = True))\n",
    "prior_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_tan(data, class_var) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tan_prediction(tan_tables, single_data_dict,outcomeSpace,class_var):\n",
    "    prob = {name : 0 for name in outcomeSpace[class_var]}\n",
    "    for key,value in tan_tables.items():\n",
    "        if (key==class_var):\n",
    "            continue\n",
    "        for name in outcomeSpace[class_var]:\n",
    "            evidence = []\n",
    "            for i in range(len(value['dom'])-1):\n",
    "                if (class_var == value['dom'][i]):\n",
    "                    evidence.append(name) \n",
    "                else:\n",
    "                    evidence.append(single_data_dict[value['dom'][i]])\n",
    "            evidence = tuple(evidence)\n",
    "            q_var = single_data_dict[value['dom'][-1]]\n",
    "            prob[name] += np.log(value['table'][evidence][q_var])\n",
    "    return (max(prob, key=prob.get))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tan_prediction(new_tan_tables,data_dict[0],outcomeSpace,class_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BreastDensity': 'high', 'Location': 'LolwOutQuad', 'Age': '35-49', 'BC': 'No', 'Mass': 'No', 'AD': 'No', 'Metastasis': 'no', 'MC': 'No', 'Size': '<1cm', 'Shape': 'Other', 'FibrTissueDev': 'No', 'LymphNodes': 'no', 'SkinRetract': 'No', 'NippleDischarge': 'No', 'Spiculation': 'No', 'Margin': 'Well-defined'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_tan(G_tan,tan_tables,test_data,outcomeSpace,class_var):\n",
    "    tan_tables = update_tan_tables(tan_tables)\n",
    "    prior = tan_tables[class_var]['table'][()]\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    var_remove = ['Metastasis', 'LymphNodes',class_var]\n",
    "    var_list = [i for i in node_list if i not in var_remove] #now we get all the variables \n",
    "    data_update = data[var_list]\n",
    "    data_dict = data_update.to_dict(orient='records')\n",
    "    outcomeSpace_copy = { var: outcomeSpace[var] for var in var_list}\n",
    "    match_count = 0\n",
    "    for i in range(len(data_dict)):\n",
    "        pred = tan_prediction(tan_tables,data_dict[i],outcomeSpace,class_var)\n",
    "        if (pred == data.iloc[i][class_var]):\n",
    "            match_count +=1\n",
    "    return (match_count/data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_tan(data,class_var):\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    fold_len = int(data.shape[0]/10)\n",
    "    acc_list = []\n",
    "#     G_tan = learn_tan_structure(data,outcomeSpace,class_var)\n",
    "    for i in range(10):\n",
    "        training_index = list(range(0,i*fold_len)) + list(range((i+1)*fold_len,data.shape[0]))\n",
    "        test_index = list(range(i*fold_len,(i+1)*fold_len))\n",
    "        training_data = data.iloc[training_index]\n",
    "        test_data = data.iloc[test_index]\n",
    "        G_tan = learn_tan_structure(training_data,outcomeSpace,class_var)\n",
    "        tan_tables = learn_bayes_net(G_tan, training_data, outcomeSpace)\n",
    "        acc_list.append(assess_tan(G_tan, tan_tables, test_data, outcomeSpace,class_var))\n",
    "    \n",
    "    print(acc_list)\n",
    "    return (np.mean(acc_list),np.std(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.816, 0.8148, 0.816, 0.8166, 0.81635, 0.81545, 0.81565, 0.8155, 0.8165, 0.81745]\n",
      "The time consumption is 156.40771341323853 seconds\n",
      "0.8160299999999999 0.00580603134679791\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "begin = time.time()\n",
    "acc, stdev = cv_tan(data,class_var)\n",
    "end = time.time()\n",
    "print(\"The time consumption is {0} seconds\".format(end-begin))\n",
    "print(acc,stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 6 - Report\n",
    "\n",
    "Write a report (**with less than 500 words**) summarising your findings in this assignment. Your report should address the following:\n",
    "\n",
    "a. Make a summary and discussion of the experimental results (accuracy). Use plots to illustrate your results.\n",
    "\n",
    "b. Discuss the complexity of the implemented algorithms.\n",
    "\n",
    "Use Markdown and Latex to write your report in the Jupyter notebook. Develop some plots using Matplotlib to illustrate your results. Be mindful of the maximum number of words. Please, be concise and objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your report in one or more cells here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
