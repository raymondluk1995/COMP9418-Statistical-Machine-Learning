{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9418 - Assignment 1 - Bayesian Networks as Classifiers\n",
    "\n",
    "## UNSW Sydney, October 2020\n",
    "\n",
    "- Student name 1 - zID\n",
    "- Student name 2 - ZID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "**Submission deadline:** Sunday, 18th October 2020, at 18:00:00.\n",
    "\n",
    "**Late Submission Policy:** The penalty is set at 20% per late day. This is ceiling penalty, so if a group is marked 60/100 and they submitted two days late, they still get 60/100.\n",
    "\n",
    "**Form of Submission:** This is a group assignment. Each group can have up to **two** students. **Only one member of the group should submit the assignment**.\n",
    "\n",
    "You can reuse any piece of source code developed in the tutorials.\n",
    "\n",
    "Submit your files using give. On a CSE Linux machine, type the following on the command-line:\n",
    "\n",
    "``$ give cs9418 ass1 solution.zip``\n",
    "\n",
    "Alternative, you can submit your solution via the [WebCMS](https://webcms3.cse.unsw.edu.au/COMP9418/20T3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical prerequisites\n",
    "\n",
    "These are the libraries your are allowed to use. No other libraries will be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make division default to floating-point, saving confusion\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Allowed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import heapq as pq\n",
    "import matplotlib as mp\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "from collections import OrderedDict as odict\n",
    "from graphviz import Digraph\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Supplemental libraries\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial task - Initialise graph\n",
    "\n",
    "Create a graph ``G`` that represents the following network by filling in the edge lists.\n",
    "![Bayes Net](BayesNet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = {\n",
    "    \"BreastDensity\" : [\"Mass\"],\n",
    "    \"Location\" : [\"BC\"],\n",
    "    \"Age\" : [\"BC\"],\n",
    "    \"BC\" : [\"Mass\", \"AD\", \"Metastasis\", \"MC\", \"SkinRetract\",\"NippleDischarge\"],\n",
    "    \"Mass\" : [\"Size\",  \"Shape\", \"Margin\" ],\n",
    "    \"AD\" : [\"FibrTissueDev\"],\n",
    "    \"Metastasis\" : [ \"LymphNodes\"],\n",
    "    \"MC\" : [],\n",
    "    \"Size\" : [],\n",
    "    \"Shape\" : [],\n",
    "    \"FibrTissueDev\" : [ \"SkinRetract\" , \"NippleDischarge\",\"Spiculation\" ],\n",
    "    \"LymphNodes\" : [],\n",
    "    \"SkinRetract\" : [],\n",
    "    \"NippleDischarge\" : [],\n",
    "    \"Spiculation\" : [\"Margin\" ],\n",
    "    \"Margin\" : [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 1 - Efficient d-separation test\n",
    "\n",
    "Implement the efficient version of the d-separation algorithm in a function ``d_separation(G, X, Z, Y)`` that return a boolean: true if **X** is d-separated from **Y** given **Z** in the graph $G$ and false otherwise.\n",
    "\n",
    "* **X**,**Y** and **Z** are python sets, each containing a set of variable names. \n",
    "* Variable names may be strings or integers, and can be assumed to be nodes of the graph $G$. \n",
    "* $G$ is a graph as defined in tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for d_separation(G, X, Z, Y) in one or more cells here\n",
    "import copy \n",
    "\n",
    "\n",
    "def isleaf_node(G,node):\n",
    "    return not G[node] \n",
    "\n",
    "def remove_leaf(G1,leaf_node):\n",
    "    # delete the leaf node and its edges from the G, return a new Graph\n",
    "    G_new = copy.deepcopy(G1)\n",
    "    del G_new[leaf_node]\n",
    "    for key, value in  G_new.items(): \n",
    "        if leaf_node in value:\n",
    "            G_new[key].remove(leaf_node)\n",
    "    return G_new\n",
    "    \n",
    "def repeat_del(G1,node_list): \n",
    "    count = 1 \n",
    "    list_update = node_list.copy()\n",
    "    while count > 0:\n",
    "        count = 0\n",
    "        for node in node_list: \n",
    "            if isleaf_node(G1,node):\n",
    "                #print(node)\n",
    "                #remove the nodes and update the graph \n",
    "                G1= copy.deepcopy(remove_leaf(G1,node))\n",
    "                #print(G)\n",
    "                list_update.remove(node)\n",
    "                count = count + 1 \n",
    "                #print(count)\n",
    "        node_list = list_update.copy()\n",
    "    return (G1)\n",
    "    \n",
    "\n",
    "    #if count == 0:\n",
    "    #    return(G)\n",
    "    #print(list_update)\n",
    "    #new_list = list_update.copy()\n",
    "    #G_new = repeat_del(G,new_list)\n",
    "    \n",
    " \n",
    "    \n",
    "def dfs_r(G, v, colour):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `G`, an adjacency list representation of a graph\n",
    "    `v`, next vertex to be visited\n",
    "    `colour`, dictionary with the colour of each node\n",
    "    \"\"\"\n",
    "    #print('Visiting: ', v)\n",
    "    # Visited vertices are coloured 'grey'\n",
    "    colour[v] = 'grey'\n",
    "    # Let's visit all outgoing edges from v\n",
    "    for w in G[v]:\n",
    "        # To avoid loops, we vist check if the next vertex hasn't been visited yet\n",
    "        if colour[w] == 'white':\n",
    "            dfs_r(G, w, colour)\n",
    "    # When we finish the for loop, we know we have visited all nodes from v. It is time to turn it 'black'\n",
    "    colour[v] = 'black' \n",
    "    return None\n",
    "  \n",
    "\n",
    "\n",
    "def d_separation(G1, X, Z, Y): \n",
    "    \"\"\" \n",
    "    Arguments: \n",
    "    `G`, an adjacency list representation of a graph \n",
    "    `X`, a set of variables name \n",
    "    `Y`, a set of variables name \n",
    "    `Z`, a set of a set of variables name \n",
    "    \n",
    "    Returns \n",
    "    a boolean: true if X is d-separated from Y given Z in the graph  ùê∫  and false otherwise.\n",
    "    \n",
    "    \"\"\"\n",
    "    if bool(X.intersection(Y).intersection(Z)):\n",
    "        print(\"X, Y, Z are not disjoint\")  #make it a warning/ error message? \n",
    "    \n",
    "    combine_set = X.union(Y).union(Z)\n",
    "    node_set = set(G1.keys())\n",
    "    remain_nodes = set(node_set  - combine_set)\n",
    "    \n",
    "    G_final = copy.deepcopy(repeat_del(G1,remain_nodes)) #repeat del leaf nodes \n",
    "    \n",
    "    for var in Z: \n",
    "        G_final[var] = [] #delete outgoing edges from Z set \n",
    "    \n",
    "    for key,values in G_final.items():\n",
    "        if bool(values):\n",
    "            for n in values: \n",
    "                G_final[n].append(key) #make it undirect graph\n",
    "\n",
    "    colour = {node: 'white' for node in G_final.keys()}\n",
    "    #check connectivity \n",
    "    separate = True \n",
    "    for nodex in X:\n",
    "        dfs_r(G_final,nodex,colour) \n",
    "        Y_color = [colour[node] for node in Y]\n",
    "        #print(Y_color)\n",
    "        if 'black' in Y_color:\n",
    "            separate = False\n",
    "            \n",
    "\n",
    "    return(separate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "def test(statement):\n",
    "    if statement:\n",
    "        print(\"Passed test case\")\n",
    "    else:\n",
    "        print(\"Failed test case\")\n",
    "        \n",
    "test(d_separation(G, set(['Age']), set(['BC']), set(['AD'])))\n",
    "test(not d_separation(G, set(['Spiculation','LymphNodes']), set(['MC', 'Size']), set(['Age'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Marks] Task 2 - Estimate Bayesian Network parameters from data\n",
    "\n",
    "Implement a function ``learn_outcome_space(data)`` that learns the outcome space (the valid values for each variable) from the pandas dataframe ``data`` and returns a dictionary ``outcomeSpace`` with these values.\n",
    "\n",
    "Implement a function ``learn_bayes_net(G, data, outcomeSpace)`` that learns the parameters of the Bayesian Network $G$. This function should return a dictionary ``prob_tables`` with the all conditional probability tables (one for each node).\n",
    "\n",
    "- ``G`` is a directed acyclic graph. For this part of the assignment, $G$ should be declared according to the breast cancer Bayesian network presented in the diagram in the assignment specification.\n",
    "- ``data`` is a dataframe created from a csv file containing the relevant data. \n",
    "- ``outcomeSpace`` is defined in tutorials.\n",
    "- ``prob_tables`` is a dict from each variable name (node) to a \"factor\". Factors are defined in tutorial 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_outcome_space(data) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_outcome_space(data):\n",
    "    outcomeSpace = {}\n",
    "    for attr in data.columns:\n",
    "        outcomeSpace[attr] = tuple(data[attr].unique())\n",
    "    return(outcomeSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "with open('bc.csv') as file:\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "outcomeSpace = learn_outcome_space(data)\n",
    "\n",
    "outcomes = outcomeSpace['BreastDensity']\n",
    "answer = ('high', 'medium', 'low')\n",
    "test(len(outcomes) == len(answer) and set(outcomes) == set(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_bayes_net(G, data, outcomeSpace) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxilliary functions\n",
    "def printFactor(f):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, a factor to print on screen\n",
    "    \"\"\"\n",
    "    # Create a empty list that we will fill in with the probability table entries\n",
    "    table = list()\n",
    "    \n",
    "    # Iterate over all keys and probability values in the table\n",
    "    for key, item in f['table'].items():\n",
    "        # Convert the tuple to a list to be able to manipulate it\n",
    "        k = list(key)\n",
    "        # Append the probability value to the list with key values\n",
    "        k.append(item)\n",
    "        # Append an entire row to the table\n",
    "        table.append(k)\n",
    "    # dom is used as table header. We need it converted to list\n",
    "    dom = list(f['dom'])\n",
    "    # Append a 'Pr' to indicate the probabity column\n",
    "    dom.append('Pr')\n",
    "    print(tabulate(table,headers=dom,tablefmt='fancy_grid'))\n",
    "    \n",
    "def prob(factor, *entry):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `factor`, a dictionary of domain and probability values,\n",
    "    `entry`, a list of values, one for each variable in the same order as specified in the factor domain.\n",
    "    \n",
    "    Returns p(entry)\n",
    "    \"\"\"\n",
    "\n",
    "    return factor['table'][entry]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allEqualThisIndex(dict_of_arrays,**fixed_vars):\n",
    "    first_array = dict_of_arrays[list(dict_of_arrays.keys())[0]]\n",
    "    index = np.ones_like(first_array,dtype=np.bool_)\n",
    "    for var_name,var_val in fixed_vars.items():\n",
    "        index = index & (np.asarray(dict_of_arrays[var_name])==var_val)\n",
    "    return (index)\n",
    "\n",
    "def estProbTable(data,var_name,parent_names,outcomeSpace,alpha=1):\n",
    "    var_outcomes = outcomeSpace[var_name]\n",
    "    parent_outcomes = [outcomeSpace[var] for var in parent_names]\n",
    "    all_parent_combinations = product(*parent_outcomes)\n",
    "    prob_table = odict()\n",
    "    \n",
    "    for i,parent_combination in enumerate(all_parent_combinations):\n",
    "        parent_vars = dict(zip(parent_names,parent_combination))\n",
    "        parent_index = allEqualThisIndex(data,**parent_vars)\n",
    "        for var_outcome in var_outcomes:\n",
    "            var_index = (np.asarray(data[var_name])==var_outcome)\n",
    "            new_dom = tuple(list(parent_combination)+[var_outcome])\n",
    "#             prob_table[new_dom]=(var_index & parent_index).sum()/parent_index.sum()\n",
    "            # Additive smoothing is applied here\n",
    "            \n",
    "            N = parent_index.sum()\n",
    "            c = (var_index & parent_index).sum()\n",
    "            X_cardinality = len(var_outcomes)\n",
    "            prob_table[new_dom] = (c+alpha)/(N+alpha*X_cardinality)\n",
    "    return({'dom':tuple(list(parent_names)+[var_name]),'table':prob_table})\n",
    "\n",
    "def transposeGraph(G):\n",
    "    GT = dict((v,[]) for v in G)\n",
    "    for v in G:\n",
    "        for w in G[v]:\n",
    "            GT[w].append(v)\n",
    "    return (GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_bayes_net(G,data,outcomeSpace):\n",
    "    bayes_net = odict()\n",
    "    GT = transposeGraph(G)\n",
    "    for child, parents in GT.items():\n",
    "        bayes_net[child] = estProbTable(data,child,parents,outcomeSpace)\n",
    "    return(bayes_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "prob_tables = learn_bayes_net(G, data, outcomeSpace)\n",
    "test(abs(prob_tables['Age']['table'][('35-49',)] - 0.2476) < 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 3 - Bayesian Network Classification\n",
    "\n",
    "Design a new function ``assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)`` that uses the test cases in ``data`` to assess the performance of the Bayesian network defined by ``G`` and ``prob_tables``. Implement the efficient classification procedure discussed in the lectures. Such a function should return the classifier accuracy. \n",
    " * ``class_var`` is the name of the variable you are predicting, using all other variables.\n",
    " * ``outcomeSpace`` was created in task 2\n",
    " \n",
    "Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy.\n",
    "\n",
    "Return just the accuracy:\n",
    "\n",
    "``acc = assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var) in one or more cells here\n",
    "def join(f1, f2, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f1`, first factor to be joined.\n",
    "    `f2`, second factor to be joined.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor with a join of f1 and f2\n",
    "    \"\"\"\n",
    "    \n",
    "    # First, we need to determine the domain of the new factor. It will be union of the domain in f1 and f2\n",
    "    # But it is important to eliminate the repetitions\n",
    "    common_vars = list(f1['dom']) + list(set(f2['dom']) - set(f1['dom']))\n",
    "    \n",
    "    # We will build a table from scratch, starting with an empty list. Later on, we will transform the list into a odict\n",
    "    table = list()\n",
    "    \n",
    "    # Here is where the magic happens. The product iterator will generate all combinations of varible values \n",
    "    # as specified in outcomeSpace. Therefore, it will naturally respect observed values\n",
    "    for entries in product(*[outcomeSpace[node] for node in common_vars]):\n",
    "        \n",
    "        # We need to map the entries to the domain of the factors f1 and f2\n",
    "        entryDict = dict(zip(common_vars, entries))\n",
    "        f1_entry = (entryDict[var] for var in f1['dom'])\n",
    "        f2_entry = (entryDict[var] for var in f2['dom'])\n",
    "        \n",
    "        # Insert your code here\n",
    "        p1 = prob(f1, *f1_entry)           # Use the fuction prob to calculate the probability in factor f1 for entry f1_entry \n",
    "        p2 = prob(f2, *f2_entry)           # Use the fuction prob to calculate the probability in factor f2 for entry f2_entry \n",
    "        \n",
    "        # Create a new table entry with the multiplication of p1 and p2\n",
    "        table.append((entries, p1 * p2))\n",
    "    return {'dom': tuple(common_vars), 'table': odict(table)}\n",
    "\n",
    "def p_joint(outcomeSpace, cond_tables):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `outcomeSpace`, dictionary with domain of each variable\n",
    "    `cond_tables`, conditional probability distributions estimated from data\n",
    "    \n",
    "    Returns a new factor with full joint distribution\n",
    "    \"\"\"    \n",
    "    \n",
    "    var_list = list(outcomeSpace.keys())\n",
    "    p = join(cond_tables[var_list[0]], cond_tables[var_list[1]], outcomeSpace)\n",
    "\n",
    "    for var in var_list[2:]:\n",
    "        p = join(p,cond_tables[var_list[var]],outcomeSpace)\n",
    "\n",
    "    return p\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_blanket(G,var):\n",
    "    \"\"\" determine the relevant varaibles given the var of interest, return a list of nodes \"\"\"\n",
    "    blanket_list = []\n",
    "    blanket_list = blanket_list + G[var] #include the children\n",
    "    children_list = blanket_list \n",
    "    GT = transposeGraph(G)\n",
    "    blanket_list = blanket_list + GT[var] #include the parents \n",
    "    \n",
    "    for node in children_list:\n",
    "        blanket_list = blanket_list + GT[node] #include spouse \n",
    "    \n",
    "    blanket_list = list(set(blanket_list))\n",
    "    blanket_list = [i for i in blanket_list if i != var]\n",
    "    return blanket_list\n",
    "    \n",
    "def p_joint_blanket(my_blanket, outcomeSpace, cond_tables):\n",
    "    var_list = my_blanket\n",
    "    \n",
    "    p = join(cond_tables[var_list[0]], cond_tables[var_list[1]], outcomeSpace)\n",
    "\n",
    "    for var in var_list[2:]:\n",
    "        p = join(p,cond_tables[var], outcomeSpace)\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BreastDensity',\n",
       " 'AD',\n",
       " 'FibrTissueDev',\n",
       " 'Mass',\n",
       " 'Location',\n",
       " 'Metastasis',\n",
       " 'Age',\n",
       " 'MC',\n",
       " 'SkinRetract',\n",
       " 'NippleDischarge']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markov_blanket(G,'BC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidence(var, e, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `var`, a valid variable identifier.\n",
    "    `e`, the observed value for var.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns dictionary with a copy of outcomeSpace with var = e\n",
    "    \"\"\"    \n",
    "    newOutcomeSpace = outcomeSpace.copy()      # Make a copy of outcomeSpace with a copy to method copy(). 1 line\n",
    "    newOutcomeSpace[var] = (e,)                # Replace the domain of variable var with a tuple with a single element e. 1 line\n",
    "    return newOutcomeSpace\n",
    "\n",
    "def marginalize(f, var, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be marginalized.\n",
    "    `var`, variable to be summed out.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor f' with dom(f') = dom(f) - {var}\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Let's make a copy of f domain and convert it to a list. We need a list to be able to modify its elements\n",
    "    new_dom = list(f['dom'])\n",
    "    \n",
    "    new_dom.remove(var)            # Remove var from the list new_dom by calling the method remove(). 1 line\n",
    "    table = list()                 # Create an empty list for table. We will fill in table from scratch. 1 line\n",
    "    for entries in product(*[outcomeSpace[node] for node in new_dom]):\n",
    "        s = 0;                     # Initialize the summation variable s. 1 line\n",
    "\n",
    "        # We need to iterate over all possible outcomes of the variable var\n",
    "        for val in outcomeSpace[var]:\n",
    "            # To modify the tuple entries, we will need to convert it to a list\n",
    "            entriesList = list(entries)\n",
    "            # We need to insert the value of var in the right position in entriesList\n",
    "            entriesList.insert(f['dom'].index(var), val)\n",
    "                      \n",
    "            p = prob(f, *tuple(entriesList))     # Calculate the probability of factor f for entriesList. 1 line\n",
    "            s = s + p                            # Sum over all values of var by accumulating the sum in s. 1 line\n",
    "            \n",
    "        # Create a new table entry with the multiplication of p1 and p2\n",
    "        table.append((entries, s))\n",
    "    return {'dom': tuple(new_dom), 'table': odict(table)}\n",
    "\n",
    "\n",
    "def normalize(f):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be normalized.\n",
    "    \n",
    "    Returns a new factor f' as a copy of f with entries that sum up to 1\n",
    "    \"\"\" \n",
    "    table = list()\n",
    "    sum = 0\n",
    "    for k, p in f['table'].items():\n",
    "        sum = sum + p\n",
    "    for k, p in f['table'].items():\n",
    "        table.append((k, p/sum))\n",
    "    return {'dom': f['dom'], 'table': odict(table)}\n",
    "\n",
    "\n",
    "def query(p, outcomeSpace, q_vars, **q_evi):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `p`, probability table to query.\n",
    "    `outcomeSpace`, dictionary will variable domains\n",
    "    `q_vars`, list of variables in query head\n",
    "    `q_evi`, dictionary of evidence in the form of variables names and values\n",
    "    \n",
    "    Returns a new factor NORMALIZED factor will all hidden variables eliminated as evidence set as in q_evi\n",
    "    \"\"\"     \n",
    "    \n",
    "    # Let's make a copy of these structures, since we will reuse the variable names\n",
    "    pm = p.copy()\n",
    "    outSpace = outcomeSpace.copy()\n",
    "    \n",
    "    # First, we set the evidence \n",
    "    for var_evi, e in q_evi.items():\n",
    "        outSpace = evidence(var_evi, e, outSpace)\n",
    "        \n",
    "    # Second, we eliminate hidden variables NOT in the query\n",
    "    for var in outSpace:\n",
    "        if not var in q_vars:\n",
    "            pm = marginalize(pm, var, outSpace)\n",
    "    return normalize(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var):\n",
    "    \n",
    "    var_blanket = markov_blanket(G,class_var)\n",
    "    blanket_without_var = copy.deepcopy(var_blanket)\n",
    "    var_blanket.append(class_var)\n",
    "    var_remove = ['Metastasis', 'LymphNodes']\n",
    "    var_list = [i for i in var_blanket if i not in var_remove] # now we get the variables that needs for inference class_var \n",
    "    \n",
    "#     begin = time.time()\n",
    "    p_table =  p_joint_blanket(var_list, outcomeSpace, prob_tables)\n",
    "#     end = time.time()\n",
    "#     print(\"The joining table costs {0:.2f} seconds\".format(end-begin))\n",
    "    \n",
    "    \n",
    "    \n",
    "    q_var = class_var\n",
    "    evidence_list = [var for var in var_list if var!=class_var]\n",
    "    data_update = data[evidence_list]\n",
    "    \n",
    "    data_dict = data_update.to_dict(orient='records')\n",
    "    outcomeSpace_copy = { var: outcomeSpace[var] for var in var_list}\n",
    "    match_count = 0\n",
    "    for i in range(len(data_dict)):\n",
    "        q_table = query(p_table, outcomeSpace_copy, q_var, **data_dict[i])\n",
    "        pred = max(q_table['table'],key=q_table['table'].get)[0]\n",
    "        if (pred == data.iloc[i][q_var]):\n",
    "            match_count +=1\n",
    "    return (match_count/data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BreastDensity': 'high',\n",
       " 'Location': 'LolwOutQuad',\n",
       " 'Age': '35-49',\n",
       " 'BC': 'No',\n",
       " 'Mass': 'No',\n",
       " 'AD': 'No',\n",
       " 'Metastasis': 'no',\n",
       " 'MC': 'No',\n",
       " 'Size': '<1cm',\n",
       " 'Shape': 'Other',\n",
       " 'FibrTissueDev': 'No',\n",
       " 'LymphNodes': 'no',\n",
       " 'SkinRetract': 'No',\n",
       " 'NippleDischarge': 'No',\n",
       " 'Spiculation': 'No',\n",
       " 'Margin': 'Well-defined'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy = data.copy()\n",
    "data_dict = data_copy.to_dict(orient='records')\n",
    "data_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "class_var = \"BC\"\n",
    "acc = assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8423"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a function ``cv_bayes_net(G, data, class_var)`` that uses ``learn_outcome_space``, ``learn_bayes_net``and ``assess_bayes_net`` to learn and assess a Bayesian network in a dataset using 10-fold cross-validation. Compute and report the average accuracy over the ten cross-validation runs as well as the standard deviation, e.g.\n",
    "\n",
    "``acc, stddev = cv_bayes_net(G, data, class_var)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_bayes_net(G, data, class_var) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross validation is 10 fold here\n",
    "def cv_bayes_net(G,data,class_var):\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    fold_len = int(data.shape[0]/10)\n",
    "    acc_list = []\n",
    "    for i in range(10):\n",
    "        training_index = list(range(0,i*fold_len)) + list(range((i+1)*fold_len,data.shape[0]))\n",
    "        test_index = list(range(i*fold_len,(i+1)*fold_len))\n",
    "        training_data = data.iloc[training_index]\n",
    "        test_data = data.iloc[test_index]\n",
    "        prob_tables = learn_bayes_net(G,training_data,outcomeSpace)\n",
    "        acc_list.append(assess_bayes_net(G,prob_tables,test_data,outcomeSpace,class_var))\n",
    "    \n",
    "    print(acc_list)\n",
    "    return (np.mean(acc_list),np.std(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.841, 0.8445, 0.836, 0.848, 0.846, 0.843, 0.8285, 0.84, 0.837, 0.848]\n",
      "The time consumption is 8.26 seconds\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "import time\n",
    "begin = time.time()\n",
    "acc, stddev = cv_bayes_net(G, data, 'BC')\n",
    "end = time.time()\n",
    "print(\"The time consumption is {0:.2f} seconds\".format(end-begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8412000000000001"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Marks] Task 4 - Na√Øve Bayes Classification\n",
    "\n",
    "Design a new function ``assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var)`` to classify and assess the test cases in a dataset ``data`` according to the Na√Øve Bayes classifier. To classify each example, use the log probability trick discussed in the lectures. This function should return the accuracy of the classifier in ``data``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var) in one or more cells here\n",
    "\n",
    "def learn_naive_bayes_structure(outcomeSpace, class_var):\n",
    "    \"\"\"Return the naive-bayes graph structure (a dict) according to above info\"\"\"\n",
    "    G_nb = {}\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    node_list.remove(class_var)\n",
    "    \n",
    "    G_nb[class_var] = node_list\n",
    "    \n",
    "    for nodes in node_list:\n",
    "        G_nb[nodes] = []\n",
    "    \n",
    "    return G_nb\n",
    "\n",
    "def additive_smoothing(prop_tables,data,alpha=1):\n",
    "    N = data.shape[0]\n",
    "    tables = copy.deepcopy(prop_tables)\n",
    "    for node , table_dict in tables.items():\n",
    "        X_cardinality = len(table_dict['table'].values())\n",
    "        if (0 in table_dict['table'].values()): # addtive smoothing required\n",
    "            for nameSpace, prob in table_dict['table'].items():\n",
    "                count_X_eq_x = int(round(prob * N,0))\n",
    "                table_dict['table'][nameSpace] = (count_X_eq_x+alpha)/(N+alpha*X_cardinality)\n",
    "    return (tables)\n",
    "\n",
    "\n",
    "def single_var_query( e, node_table):\n",
    "    '''Return the log likelihood for each evidence variable '''\n",
    "    prob_with_evi = {key[0]: np.log(value) for key,value in node_table['table'].items() if key[1] == e} #np.log avoid error for log(0),, but it is slower\n",
    "    \n",
    "    return prob_with_evi\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def predict(x, y_space, table, prior):\n",
    "    # initialize the prediction dictionary by the prior probabilities\n",
    "    pre_dict = {i:  np.log(prior[i]) for i in y_space if prior[i]!=0}\n",
    "    for i in range(len(x)):\n",
    "        var_prob = single_var_query(x[i], table[x.index[i]])\n",
    "        for key in pre_dict.keys():\n",
    "            pre_dict[key] = pre_dict[key] +  var_prob[key] \n",
    "        \n",
    "    yhat = max(pre_dict, key=pre_dict.get)\n",
    "    \n",
    "    return yhat \n",
    "\n",
    "\n",
    "\n",
    "def assess_naive_bayes(G_naive, naive_tables, data, outcomeSpace, class_var):\n",
    "    naive_tables = additive_smoothing(naive_tables,data)\n",
    "    \n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    var_remove = ['Metastasis', 'LymphNodes']\n",
    "    var_list = [i for i in node_list if i not in var_remove] #now we get all the variables \n",
    "    \n",
    "    evidence_list = [var for var in var_list if var!=class_var]\n",
    "    data_update = data[evidence_list]\n",
    "    \n",
    "    prior_prob = data[class_var].value_counts(normalize = True)   \n",
    "    y_hat_series = data_update.apply(predict, y_space = outcomeSpace[class_var], table = naive_tables, prior = prior_prob, axis = 1)\n",
    "    correct_predict = np.sum(data[class_var] == y_hat_series)\n",
    "    acc = correct_predict/len( y_hat_series)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7926"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "G_naive = learn_naive_bayes_structure(outcomeSpace,class_var)\n",
    "naive_tables = learn_bayes_net(G_naive, data, outcomeSpace)\n",
    "acc = assess_naive_bayes(G_naive, naive_tables, data, outcomeSpace, 'BC')\n",
    "acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a new function ``cv_naive_bayes(data, class_var)`` that uses ``assess_naive_bayes`` to assess the performance of the Na√Øve Bayes classifier in a dataset ``data``. To develop this code, perform the following steps:\n",
    "\n",
    "1. Use 10-fold cross-validation to split the data into training and test sets.\n",
    "\n",
    "2. Implement a function ``learn_naive_bayes_structure(outcomeSpace, class_var)`` to create and return a Na√Øve Bayes graph structure from ``outcomeSpace`` and ``class_var``. \n",
    "\n",
    "3. Use ``learn_bayes_net(G, data, outcomeSpace)`` to learn the Na√Øve Bayes parameters from a training set ``data``. \n",
    "\n",
    "4. Use ``assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var)`` to compute the accuracy of the Na√Øve Bayes classifier in a test set ``data``. Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy.\n",
    "\n",
    "Do 10-fold cross-validation, same as above, and return ``acc`` and ``stddev``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_naive_bayes(data, class_var) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_naive_bayes(data,class_var):\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    fold_len = int(data.shape[0]/10)\n",
    "    acc_list = []\n",
    "    G_naive = learn_naive_bayes_structure(outcomeSpace,class_var)\n",
    "    for i in range(10):\n",
    "        training_index = list(range(0,i*fold_len)) + list(range((i+1)*fold_len,data.shape[0]))\n",
    "        test_index = list(range(i*fold_len,(i+1)*fold_len))\n",
    "        training_data = data.iloc[training_index]\n",
    "        test_data = data.iloc[test_index]\n",
    "        naive_tables = learn_bayes_net(G_naive, training_data, outcomeSpace)\n",
    "        acc_list.append(assess_naive_bayes(G_naive, naive_tables, test_data, outcomeSpace,class_var))\n",
    "    return (np.mean(acc_list),np.std(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time consumption is 12.570412874221802 seconds\n",
      "0.7919499999999999 0.00580603134679791\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "begin = time.time()\n",
    "acc, stdev = cv_naive_bayes(data,class_var)\n",
    "end = time.time()\n",
    "print(\"The time consumption is {0} seconds\".format(end-begin))\n",
    "print(acc,stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 5 - Tree-augmented Na√Øve Bayes Classification\n",
    "\n",
    "Similarly to the previous task, implement a Tree-augmented Na√Øve Bayes (TAN) classifier and evaluate your implementation in the breast cancer dataset. Design a function ``learn_tan_structure(data, outcomeSpace, class_var)`` to learn the TAN structure (graph) from the ``data`` and returns such a structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note by Yangqi : The following functions are used to enable more efficient (maybe) computation for mutual information \n",
    "\n",
    "To calculate conditional mutual info: \n",
    "https://en.wikipedia.org/wiki/Conditional_mutual_information#Some_identities\n",
    "\n",
    "\n",
    "$I(X;Y|Z) = H(X|Z) + H(Y|Z) - H(X,Y|Z)$\n",
    "\n",
    "$\\mathrm{H}(Y \\mid X)=-\\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} p(x, y) \\log \\frac{p(x, y)}{p(x)}$\n",
    "\n",
    "using the Entropy identity: we can calculate it easily. To calculate entropy, we need to know the distribution for each jotint var, \n",
    "df.value_counts will be usful \n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.value_counts.html\n",
    "\n",
    "Then we simply calculate entropy and workout the conditional mutual info. \n",
    "\n",
    "https://stackoverflow.com/questions/49685591/how-to-find-the-entropy-of-each-column-of-data-set-by-python\n",
    "\n",
    "After that, Bayesian graph can be quickly obtain by max span tree \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_entro(var_list,class_var,data):\n",
    "    probs_class_var = dict(data[class_var].value_counts(normalize=True,sort=False))\n",
    "    var_list.append(class_var)\n",
    "    probs_join_var = dict(data.groupby(var_list).size())\n",
    "    join_sum = sum(probs_join_var.values())\n",
    "    probs_join_var = {key:value/join_sum for key,value in probs_join_var.items()}\n",
    "    entropy = 0 \n",
    "    for key,value in probs_join_var.items():\n",
    "        entropy -= value * np.log(value/probs_class_var[key[-1]])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MI(var1,var2,class_var,data):\n",
    "    mutual_info = cond_entro([var1],class_var,data) + \\\n",
    "                  cond_entro([var2],class_var,data) - \\\n",
    "                  cond_entro([var1,var2],class_var,data)\n",
    "    return(mutual_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isCyclicUtil(v,visited,parent,MST):\n",
    "    visited[v] = True\n",
    "    \n",
    "    for node in MST[v]:\n",
    "        if (visited[node]==False):\n",
    "            if(isCyclicUtil(node,visited,v,MST)):\n",
    "                return True\n",
    "        elif(parent!=node):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def isCyclic(MST,node_list):\n",
    "    visited = {node:False for node in node_list}\n",
    "    \n",
    "    for node in node_list:\n",
    "        if (visited[node]==False):\n",
    "            if(isCyclicUtil(node,visited,None,MST)==True):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def max_spanning_tree(input_edges,outcomeSpace,class_var):\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    node_list.remove(\"Metastasis\")\n",
    "    node_list.remove(\"LymphNodes\")\n",
    "    MST = {node:[] for node in node_list}\n",
    "    node_list.remove(class_var)\n",
    "    edge_count = 0\n",
    "    edges = copy.deepcopy(input_edges)\n",
    "    edges.sort(reverse=True,key=lambda x:x[1])\n",
    "    \n",
    "    while(len(edges)>0):\n",
    "        edge = edges.pop(0)[0]\n",
    "        MST[edge[0]].append(edge[1])\n",
    "        MST[edge[1]].append(edge[0])\n",
    "        edge_count +=1\n",
    "        \n",
    "        if (isCyclic(MST,node_list)):\n",
    "            MST[edge[0]].remove(edge[1])\n",
    "            MST[edge[1]].remove(edge[0])\n",
    "            edge_count -=1\n",
    "        \n",
    "        if (edge_count == len(node_list)-1):\n",
    "            return (MST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_directed_MST(G_tan,MST,node,node_count):\n",
    "    children = MST[node]\n",
    "    for child in children:\n",
    "        MST[child].remove(node)\n",
    "        G_tan,node_count = generate_directed_MST(G_tan,MST,child,node_count)\n",
    "        G_tan[node].append(child) \n",
    "    node_count+=1\n",
    "    return (G_tan,node_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_tan_structure(data,outcomeSpace,class_var):\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    node_list.remove(\"Metastasis\")\n",
    "    node_list.remove(\"LymphNodes\")\n",
    "    edges = []\n",
    "    G_tan = {node:[] for node in node_list}\n",
    "    node_list.remove(class_var)\n",
    "    for i in range(len(node_list)-1):\n",
    "        for j in range(i+1,len(node_list)):\n",
    "            mutual_info = MI(node_list[i],node_list[j],class_var,data)\n",
    "            edges.append(((node_list[i],node_list[j]),mutual_info))\n",
    "    MST = max_spanning_tree(edges,outcomeSpace,class_var)\n",
    "    node_count = 0\n",
    "    for node in node_list:\n",
    "        if (node_count == len(node_list)):\n",
    "            break\n",
    "        G_tan,node_count = generate_directed_MST(G_tan,MST,node,node_count)\n",
    "    G_tan[class_var] = node_list\n",
    "    return(G_tan)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time consumption is 1.58 seconds\n",
      "Passed test case\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "############\n",
    "## TEST CODE\n",
    "begin = time.time()\n",
    "tan_graph = learn_tan_structure(data, outcomeSpace, class_var)\n",
    "end = time.time()\n",
    "print(\"The time consumption is {0:.2f} seconds\".format(end-begin))\n",
    "test(len(tan_graph['BC']) == len(tan_graph)-1)\n",
    "test('FibrTissueDev' in tan_graph['Spiculation'] or 'Spiculation' in tan_graph['FibrTissueDev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the other tasks, design a function ``cv_tan(data, class_var)`` that uses 10-fold cross-validation to assess the performance of the TAN classifier from ``data``. Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy. This function should use the ``learn_tan_structure`` as well as other functions defined in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_tan(data, class_var) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tan_tables(tan_tables):\n",
    "    new_table = odict()\n",
    "    for key, value in tan_tables.items(): #key is the var name and value is its dictionary\n",
    "        new_sub_table = odict()\n",
    "        for sub_key, sub_value in value['table'].items():\n",
    "            new_sub_key = tuple(list(sub_key)[:-1])\n",
    "            q_var = sub_key[-1]\n",
    "            if (new_sub_key not in new_sub_table):\n",
    "                new_sub_table[new_sub_key] = odict()\n",
    "            new_sub_table[new_sub_key][q_var] = sub_value\n",
    "        new_table[key] = {'dom':value['dom'],'table':new_sub_table}\n",
    "    return (new_table)\n",
    "\n",
    "def tan_prediction(tan_tables, single_data_dict,outcomeSpace,class_var):\n",
    "    prob = {name : 0 for name in outcomeSpace[class_var]}\n",
    "    for key,value in tan_tables.items():\n",
    "        if (key==class_var):\n",
    "            continue\n",
    "        for name in outcomeSpace[class_var]:\n",
    "            evidence = []\n",
    "            for i in range(len(value['dom'])-1):\n",
    "                if (class_var == value['dom'][i]):\n",
    "                    evidence.append(name) \n",
    "                else:\n",
    "                    evidence.append(single_data_dict[value['dom'][i]])\n",
    "            evidence = tuple(evidence)\n",
    "            q_var = single_data_dict[value['dom'][-1]]\n",
    "            prob[name] += np.log(value['table'][evidence][q_var])\n",
    "    return (max(prob, key=prob.get))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_tan(G_tan,tan_tables,test_data,outcomeSpace,class_var):\n",
    "    tan_tables = update_tan_tables(tan_tables)\n",
    "    prior = tan_tables[class_var]['table'][()]\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    var_remove = ['Metastasis', 'LymphNodes',class_var]\n",
    "    var_list = [i for i in node_list if i not in var_remove] #now we get all the variables \n",
    "    data_update = data[var_list]\n",
    "    data_dict = data_update.to_dict(orient='records')\n",
    "    outcomeSpace_copy = { var: outcomeSpace[var] for var in var_list}\n",
    "    match_count = 0\n",
    "    for i in range(len(data_dict)):\n",
    "        pred = tan_prediction(tan_tables,data_dict[i],outcomeSpace,class_var)\n",
    "        if (pred == data.iloc[i][class_var]):\n",
    "            match_count +=1\n",
    "    return (match_count/data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_tan(data,class_var):\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    fold_len = int(data.shape[0]/10)\n",
    "    acc_list = []\n",
    "#     G_tan = learn_tan_structure(data,outcomeSpace,class_var)\n",
    "    for i in range(10):\n",
    "        training_index = list(range(0,i*fold_len)) + list(range((i+1)*fold_len,data.shape[0]))\n",
    "        test_index = list(range(i*fold_len,(i+1)*fold_len))\n",
    "        training_data = data.iloc[training_index]\n",
    "        test_data = data.iloc[test_index]\n",
    "        G_tan = learn_tan_structure(training_data,outcomeSpace,class_var)\n",
    "        tan_tables = learn_bayes_net(G_tan, training_data, outcomeSpace)\n",
    "        acc_list.append(assess_tan(G_tan, tan_tables, test_data, outcomeSpace,class_var))\n",
    "    \n",
    "    print(acc_list)\n",
    "    return (np.mean(acc_list),np.std(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.816, 0.8148, 0.816, 0.8166, 0.81635, 0.81545, 0.81565, 0.8155, 0.8165, 0.81745]\n",
      "The time consumption is 66.71268606185913 seconds\n",
      "0.8160299999999999 0.00580603134679791\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "begin = time.time()\n",
    "acc, stdev = cv_tan(data,class_var)\n",
    "end = time.time()\n",
    "print(\"The time consumption is {0} seconds\".format(end-begin))\n",
    "print(acc,stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 6 - Report\n",
    "\n",
    "Write a report (**with less than 500 words**) summarising your findings in this assignment. Your report should address the following:\n",
    "\n",
    "a. Make a summary and discussion of the experimental results (accuracy). Use plots to illustrate your results.\n",
    "\n",
    "b. Discuss the complexity of the implemented algorithms.\n",
    "\n",
    "Use Markdown and Latex to write your report in the Jupyter notebook. Develop some plots using Matplotlib to illustrate your results. Be mindful of the maximum number of words. Please, be concise and objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your report in one or more cells here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro \n",
    "\n",
    "In this report, we investigate the prediction of breast cancer using various Bayesian network model. Breast cancer are related to many different variables, such as age, location and information about X-ray image . Our results suggest that Tree-augument Naive Bayes network is preferred compared to Bayesian netowrk classification and Naive Bayes method, considering the serious effect of false negative, although the accuracy is slighly lower than Bayesian classification network. Under 10-fold cross validation, the accuracy of Tree-augument Naive Bayes is around 83.3% while the accuracy  \n",
    "\n",
    "\n",
    "## Three methods \n",
    "\n",
    "1. Bayesian classificantion network \n",
    "\n",
    "Bayesian classification network is built according to expert's opnion about relevant features that will influence the likelihood of breast cancer. In this model, the variables's casual relationship is derived from medical research theory. This will lead to an automatic variable selection process by Markov blanket, with given evidence. For example, in our netwrok G, variables such as Size, Shape and Margin are removed from this calssification model. This method is preferred when the causal relationship between variables are well-established or when we are having too many variables in the model.  \n",
    "\n",
    "\n",
    "2. Naive Bayes \n",
    "\n",
    "Naive Bayes assume all the features are indepedent so the joint distribution of $p(y,x_1,\\cdots, x_m) = p(y) \\Pi_i p(x_i|y)$ becomes a mutliplication of m conditional distribution with the prior. This significantly reduce the number of parameters from exponential to linear. However, the indepedent assumption could be questionable in the model.\n",
    "\n",
    "3. Tree-augument Bayes network \n",
    "\n",
    "Tree-augument Bayes network (TAN) allows the dependency between features by calculating the conditional mutual information between each pair of variables and establish the dependency relationship according to maximum spanning tree algorithm. Since only one parent is allowed amongst features, the parametrs space is growing from linear to quadratic level, which enables efficient computation without indepdent assumption.\n",
    "\n",
    "\n",
    "\n",
    "## Results and discussion \n",
    "\n",
    "The classifcation variable BC has three categories$\\{c_1, c_2, c_3\\}= $ {'Insitu', 'Invasive', 'No'}. After develop the models, the inquiry is $\\hat{c} = \\arg_{c_i} \\max p(c_i, e)$ and the accuracy is calculated by $acc = \\frac{\\sum I(\\hat{c_i} = c_i)}{N} $ The results and the computation time with 10-fold validation is given in figure 1. \n",
    "\n",
    "\n",
    "According to the results, it is obvious that Bayes network classifcation is outperforming the other two models with much greater accuracy but also lower computation time.\n",
    "We also check the perforamnce in each fold to ensure the model is not overfitting. \n",
    "\n",
    "\n",
    "Another important aspect of detection of breast cancer is about assignning different weights to false negative and false positive. Since there are three categories of BC, the accuracy in each categories are reported. \n",
    "\n",
    "Bayes networks \n",
    "\n",
    "Naive Bayes \n",
    "\n",
    "TAN\n",
    "\n",
    "False negative is a much more serious issue than False negative in medical detection. In this case, the false negative includes (BC =Insitu, prediction = No),  (BC =Invasive, prediction = No); as they will lead to no treatment for those patient.\n",
    "According to the table above, we can see some advantages of TAN compared to the other two model. \n",
    "For patient with Insitu, TAN has the lowest likelihood to predict as No and for patient with Invasive, TAN also has the lowest likelihood to predict them as NO.\n",
    "Therefore, despite of slighly lower total accuracy of TAN, it is still preferred when we consider the serious outcome of false negative.\n",
    "\n",
    "\n",
    "## Complexity and computation time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = {'Accuracy': [0.84,0.79,0.83], 'Time': [6.16, 7.92,57.27]}\n",
    "df = pd.DataFrame.from_dict(results)\n",
    "df.index = ['Bayes net', \"Naive Bayes\", \"TAN\"]\n",
    "\n",
    "\n",
    "fig = plt.figure() # Create matplotlib figure\n",
    "\n",
    "ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.\n",
    "\n",
    "width = 0.25\n",
    "\n",
    "df.Accuracy.plot(kind='bar', color='red', ax=ax, width=width, position=1)\n",
    "df.Time.plot(kind='bar', color='blue', ax=ax2, width=width, position=0)\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax2.set_ylabel('Computation (sec)')\n",
    "\n",
    "ax.set_ylim([0,1])\n",
    "ax2.set_ylim([0,80])\n",
    "\n",
    "plt.title('Model comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_bayes_net2(G, prob_tables, data, outcomeSpace, class_var):\n",
    "    \n",
    "    var_blanket = markov_blanket(G,class_var)\n",
    "    blanket_without_var = copy.deepcopy(var_blanket)\n",
    "    var_blanket.append(class_var)\n",
    "    var_remove = ['Metastasis', 'LymphNodes']\n",
    "    var_list = [i for i in var_blanket if i not in var_remove] # now we get the variables that needs for inference class_var \n",
    "    \n",
    "#     begin = time.time()\n",
    "    p_table =  p_joint_blanket(var_list, outcomeSpace, prob_tables)\n",
    "#     end = time.time()\n",
    "#     print(\"The joining table costs {0:.2f} seconds\".format(end-begin))\n",
    "    \n",
    "    \n",
    "    \n",
    "    q_var = class_var\n",
    "    evidence_list = [var for var in var_list if var!=class_var]\n",
    "    data_update = data[evidence_list]\n",
    "    \n",
    "    data_dict = data_update.to_dict(orient='records')\n",
    "    outcomeSpace_copy = { var: outcomeSpace[var] for var in var_list}\n",
    "    match_count = 0\n",
    "    pred_list = []\n",
    "    for i in range(len(data_dict)):\n",
    "        q_table = query(p_table, outcomeSpace_copy, q_var, **data_dict[i])\n",
    "        pred = max(q_table['table'],key=q_table['table'].get)[0]\n",
    "        pred_list.append(pred)\n",
    "    return (pred_list)\n",
    "\n",
    "def assess_naive_bayes2(G_naive, naive_tables, data, outcomeSpace, class_var):\n",
    "    naive_tables = additive_smoothing(naive_tables,data)\n",
    "    \n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    var_remove = ['Metastasis', 'LymphNodes']\n",
    "    var_list = [i for i in node_list if i not in var_remove] #now we get all the variables \n",
    "    \n",
    "    evidence_list = [var for var in var_list if var!=class_var]\n",
    "    data_update = data[evidence_list]\n",
    "    \n",
    "    prior_prob = data[class_var].value_counts(normalize = True)   \n",
    "    y_hat_series = data_update.apply(predict, y_space = outcomeSpace[class_var], table = naive_tables, prior = prior_prob, axis = 1)\n",
    "    #correct_predict = np.sum(data[class_var] == y_hat_series)\n",
    "    #acc = correct_predict/len( y_hat_series)\n",
    "    \n",
    "    return  y_hat_series\n",
    "\n",
    "def assess_tan2(G_tan,tan_tables,test_data,outcomeSpace,class_var):\n",
    "    tan_tables = update_tan_tables(tan_tables)\n",
    "    prior = tan_tables[class_var]['table'][()]\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "    var_remove = ['Metastasis', 'LymphNodes',class_var]\n",
    "    var_list = [i for i in node_list if i not in var_remove] #now we get all the variables \n",
    "    data_update = data[var_list]\n",
    "    data_dict = data_update.to_dict(orient='records')\n",
    "    outcomeSpace_copy = { var: outcomeSpace[var] for var in var_list}\n",
    "    match_count = 0\n",
    "    pred_list = []\n",
    "    for i in range(len(data_dict)):\n",
    "        pred = tan_prediction(tan_tables,data_dict[i],outcomeSpace,class_var)\n",
    "        pred_list.append(pred)\n",
    "        if (pred == data.iloc[i][class_var]):\n",
    "            match_count +=1\n",
    "    return (pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_bayes =  np.array(assess_bayes_net2(G, prob_tables, data, outcomeSpace, class_var))\n",
    "data['yhat_bayes'] = yhat_bayes \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_naive = assess_naive_bayes2(G_naive, naive_tables, data, outcomeSpace, 'BC')\n",
    "data['yhat_naive'] = yhat_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_tan = learn_tan_structure(data,outcomeSpace,class_var)\n",
    "tan_tables = learn_bayes_net(G_tan,data, outcomeSpace)\n",
    "yhat_tan = assess_tan2(G_tan,tan_tables,data,outcomeSpace,class_var)\n",
    "data['yhat_tan'] = yhat_tan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prediction</th>\n",
       "      <th>Insitu</th>\n",
       "      <th>Invasive</th>\n",
       "      <th>No</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Insitu</th>\n",
       "      <td>0.338971</td>\n",
       "      <td>0.421071</td>\n",
       "      <td>0.239958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Invasive</th>\n",
       "      <td>0.106924</td>\n",
       "      <td>0.789329</td>\n",
       "      <td>0.103748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.014390</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.977249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.251350</td>\n",
       "      <td>0.666350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prediction    Insitu  Invasive        No\n",
       "True                                    \n",
       "Insitu      0.338971  0.421071  0.239958\n",
       "Invasive    0.106924  0.789329  0.103748\n",
       "No          0.014390  0.008361  0.977249\n",
       "All         0.082300  0.251350  0.666350"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(data['BC'],data['yhat_bayes'], rownames = ['True'],colnames = ['Prediction'], margins = True, normalize = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prediction</th>\n",
       "      <th>Insitu</th>\n",
       "      <th>Invasive</th>\n",
       "      <th>No</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Insitu</th>\n",
       "      <td>0.392882</td>\n",
       "      <td>0.396054</td>\n",
       "      <td>0.211064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Invasive</th>\n",
       "      <td>0.146729</td>\n",
       "      <td>0.723904</td>\n",
       "      <td>0.129367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.083930</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>0.909880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>0.142600</td>\n",
       "      <td>0.231000</td>\n",
       "      <td>0.626400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prediction    Insitu  Invasive        No\n",
       "True                                    \n",
       "Insitu      0.392882  0.396054  0.211064\n",
       "Invasive    0.146729  0.723904  0.129367\n",
       "No          0.083930  0.006190  0.909880\n",
       "All         0.142600  0.231000  0.626400"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(data['BC'],data['yhat_naive'] , rownames = ['True'],colnames = ['Prediction'], margins = True, normalize = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prediction</th>\n",
       "      <th>Insitu</th>\n",
       "      <th>Invasive</th>\n",
       "      <th>No</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Insitu</th>\n",
       "      <td>0.555673</td>\n",
       "      <td>0.299859</td>\n",
       "      <td>0.144468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Invasive</th>\n",
       "      <td>0.227610</td>\n",
       "      <td>0.698920</td>\n",
       "      <td>0.073470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.072835</td>\n",
       "      <td>0.006914</td>\n",
       "      <td>0.920251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>0.177900</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.610200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Prediction    Insitu  Invasive        No\n",
       "True                                    \n",
       "Insitu      0.555673  0.299859  0.144468\n",
       "Invasive    0.227610  0.698920  0.073470\n",
       "No          0.072835  0.006914  0.920251\n",
       "All         0.177900  0.211900  0.610200"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(data['BC'],data['yhat_tan'] , rownames = ['True'],colnames = ['Prediction'], margins = True, normalize = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
